{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEzdnJOAZngD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc70bbc-6e6e-4b11-93d8-b4571ded623b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 118287, Val images: 5000\n",
            "‚úÖ Dataset cleanup and organization complete.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "# Step 2: Setup folders\n",
        "import os, shutil, json\n",
        "from glob import glob\n",
        "\n",
        "os.makedirs(\"COCO/images/train2017\", exist_ok=True)\n",
        "os.makedirs(\"COCO/images/val2017\", exist_ok=True)\n",
        "os.makedirs(\"COCO/labels/train2017\", exist_ok=True)\n",
        "os.makedirs(\"COCO/labels/val2017\", exist_ok=True)\n",
        "\n",
        "# Step 3: Download the COCO dataset (train images, validation images, and annotations)\n",
        "!wget -q -O train2017.zip http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget -q -O val2017.zip http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget -q -O annotations_trainval2017.zip http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "# Step 4: Extract the COCO dataset\n",
        "!unzip -q train2017.zip -d COCO/images\n",
        "!unzip -q val2017.zip -d COCO/images\n",
        "!unzip -q annotations_trainval2017.zip -d COCO\n",
        "\n",
        "# Step 5: Remove zip files to save space\n",
        "os.remove('train2017.zip')\n",
        "os.remove('val2017.zip')\n",
        "os.remove('annotations_trainval2017.zip')\n",
        "\n",
        "# Step 6: Load annotations\n",
        "with open('COCO/annotations/instances_train2017.json') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('COCO/annotations/instances_val2017.json') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "def create_labels(data, split):\n",
        "    image_dict = {img['id']: img for img in data['images']}\n",
        "    label_folder = f'COCO/labels/{split}'\n",
        "    os.makedirs(label_folder, exist_ok=True)\n",
        "\n",
        "    for annotation in data['annotations']:\n",
        "        image_id = annotation['image_id']\n",
        "        image_info = image_dict.get(image_id)\n",
        "        if image_info is None:\n",
        "            continue\n",
        "\n",
        "        image_name = image_info['file_name']\n",
        "        img_w, img_h = image_info['width'], image_info['height']\n",
        "\n",
        "        bbox = annotation['bbox']\n",
        "        class_id = int(annotation['category_id']) - 1  # üëà Cast to int explicitly\n",
        "\n",
        "        if bbox[2] <= 0 or bbox[3] <= 0:\n",
        "            continue  # skip invalid boxes\n",
        "\n",
        "        x_center = (bbox[0] + bbox[2] / 2) / img_w\n",
        "        y_center = (bbox[1] + bbox[3] / 2) / img_h\n",
        "        width = bbox[2] / img_w\n",
        "        height = bbox[3] / img_h\n",
        "\n",
        "        if width <= 0 or height <= 0:\n",
        "            continue\n",
        "\n",
        "        label_file = os.path.join(label_folder, image_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        with open(label_file, 'a') as f:\n",
        "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "create_labels(train_data, 'train2017')\n",
        "create_labels(val_data, 'val2017')\n",
        "\n",
        "\n",
        "# Step 8: Copy images to the appropriate folders (train/val)\n",
        "# Step 8: Copy images to the appropriate folders (train/val)\n",
        "\n",
        "\n",
        "\n",
        "# Step 9: Cleanup annotation folder\n",
        "shutil.rmtree('COCO/annotations')\n",
        "\n",
        "# Step 10: Generate image path lists\n",
        "train_images = sorted(glob('COCO/images/train2017/*.jpg'))\n",
        "val_images = sorted(glob('COCO/images/val2017/*.jpg'))\n",
        "\n",
        "with open('train2017.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train_images))\n",
        "\n",
        "with open('val2017.txt', 'w') as f:\n",
        "    f.write('\\n'.join(val_images))\n",
        "\n",
        "print(f\"Train images: {len(train_images)}, Val images: {len(val_images)}\")\n",
        "\n",
        "# Step 11: Optional cleanup\n",
        "# shutil.rmtree('COCO/images/train2017')  # Uncomment to free up space\n",
        "\n",
        "print(\"‚úÖ Dataset cleanup and organization complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LERAFhy_24H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('COCO/labels/train2017/000000000009.txt') as f:\n",
        "    print(f.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K39ZkFzF24Kc",
        "outputId": "df9d0ebd-83c6-4915-cb91-4613dcfe0eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 0.479492 0.688771 0.955609 0.595500\n",
            "50 0.736516 0.247188 0.498875 0.476417\n",
            "55 0.637063 0.732938 0.494125 0.510583\n",
            "50 0.339438 0.418896 0.678875 0.781500\n",
            "54 0.646836 0.132552 0.118047 0.096937\n",
            "54 0.773148 0.129802 0.090734 0.097229\n",
            "54 0.668297 0.226906 0.131281 0.146896\n",
            "54 0.642859 0.079219 0.148063 0.148062\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_path = 'COCO/images/train2017/000000000009.jpg'\n",
        "print(\"Exists?\" , os.path.exists(img_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzT3IsFS24M1",
        "outputId": "c72b6dac-31d6-4b1f-d873-43fbb7844b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yac4MVKd24O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjwfA6ql24Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfZ36moyWas9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ 1) Install PyTorch 1.13.1 + CUDA¬†11.7 and other deps ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install opencv-python==4.5.5.64 PyYAML tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190389ba-ccd5-410e-b1d7-c44f83294b54",
        "id": "cZxCc7y-Wb2N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.1+cu117 (from versions: 0.1.6, 0.2.0, 0.15.0+cu117, 0.15.1, 0.15.1+cu117, 0.15.2, 0.15.2+cu117, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.1+cu117\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting opencv-python==4.5.5.64\n",
            "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python==4.5.5.64) (2.0.2)\n",
            "Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.11.0.86\n",
            "    Uninstalling opencv-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-4.11.0.86\n",
            "Successfully installed opencv-python-4.5.5.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWjr_4I23Wp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚îÄ‚îÄ‚îÄ 1) Install PyTorch 1.13.1 + CUDA¬†11.7 and other deps ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install opencv-python==4.5.5.64 PyYAML tqdm\n"
      ],
      "metadata": {
        "id": "TacLG-33Z3BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2135183-4098-4450-f4c0-5399a58a8111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Using cached https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.1+cu117 (from versions: 0.1.6, 0.2.0, 0.15.0+cu117, 0.15.1, 0.15.1+cu117, 0.15.2, 0.15.2+cu117, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.1+cu117\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: opencv-python==4.5.5.64 in /usr/local/lib/python3.11/dist-packages (4.5.5.64)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python==4.5.5.64) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "QC3v5W8We8Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b22bab-dbb6-4543-b095-4438b64b59c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m623.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.15 requires torchvision, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.13.3 torch-2.7.0 triton-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchvision>=0.6.0"
      ],
      "metadata": {
        "id": "FwHzbtCpgZqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac30d5f3-ecc0-43a8-9cfe-41fa677dfe28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy"
      ],
      "metadata": {
        "id": "zXrSslEdgxqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498abf1d-a5f4-401a-abd0-cc086ece91ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade to the newest opencv-python wheel (built against NumPy 2.x)\n",
        "!pip install --upgrade opencv-python\n"
      ],
      "metadata": {
        "id": "ehQYwA7HhZpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6ce6b5-23ce-4eac-8552-ed4f6aaa20bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.5.5.64)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.2.5)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.5.5.64\n",
            "    Uninstalling opencv-python-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-4.5.5.64\n",
            "Successfully installed opencv-python-4.11.0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZxCOqWqrBHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SITeSYo5rBKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qmw3U_mrBNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils"
      ],
      "metadata": {
        "id": "HZ8Ny3sxPCfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(\"utils\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "gbtFb1qUqSqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##args"
      ],
      "metadata": {
        "id": "940qoVLm42AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/args.yaml\n",
        "lr0: 0.010                    # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
        "lrf: 0.010                    # final OneCycleLR learning rate (lr0 * lrf)\n",
        "momentum: 0.93700000          # SGD momentum/Adam beta1\n",
        "weight_decay: 0.0005          # optimizer weight decay 5e-4\n",
        "warmup_epochs: 3.000          # warmup epochs\n",
        "warmup_momentum: 0.8          # warmup initial momentum\n",
        "warmup_bias_lr: 0.10          # warmup initial bias lr\n",
        "box: 7.5                      # box loss gain\n",
        "cls: 0.5                      # cls loss gain\n",
        "dfl: 1.5                      # cls loss gain\n",
        "hsv_h: 0.015000               # image HSV-Hue augmentation (fraction)\n",
        "hsv_s: 0.700000               # image HSV-Saturation augmentation (fraction)\n",
        "hsv_v: 0.400000               # image HSV-Value augmentation (fraction)\n",
        "degrees: 0.0000               # image rotation (+/- deg)\n",
        "translate: 0.10               # image translation (+/- fraction)\n",
        "scale: 0.500000               # image scale (+/- gain)\n",
        "shear: 0.000000               # image shear (+/- deg)\n",
        "flip_ud: 0.0000               # image flip up-down (probability)\n",
        "flip_lr: 0.5000               # image flip left-right (probability)\n",
        "mosaic: 1.00000               # image mosaic (probability)\n",
        "mix_up: 0.00000               # image mix-up (probability)\n",
        "# classes\n",
        "names:\n",
        "  0: person\n",
        "  1: bicycle\n",
        "  2: car\n",
        "  3: motorcycle\n",
        "  4: airplane\n",
        "  5: bus\n",
        "  6: train\n",
        "  7: truck\n",
        "  8: boat\n",
        "  9: traffic light\n",
        "  10: fire hydrant\n",
        "  11: stop sign\n",
        "  12: parking meter\n",
        "  13: bench\n",
        "  14: bird\n",
        "  15: cat\n",
        "  16: dog\n",
        "  17: horse\n",
        "  18: sheep\n",
        "  19: cow\n",
        "  20: elephant\n",
        "  21: bear\n",
        "  22: zebra\n",
        "  23: giraffe\n",
        "  24: backpack\n",
        "  25: umbrella\n",
        "  26: handbag\n",
        "  27: tie\n",
        "  28: suitcase\n",
        "  29: frisbee\n",
        "  30: skis\n",
        "  31: snowboard\n",
        "  32: sports ball\n",
        "  33: kite\n",
        "  34: baseball bat\n",
        "  35: baseball glove\n",
        "  36: skateboard\n",
        "  37: surfboard\n",
        "  38: tennis racket\n",
        "  39: bottle\n",
        "  40: wine glass\n",
        "  41: cup\n",
        "  42: fork\n",
        "  43: knife\n",
        "  44: spoon\n",
        "  45: bowl\n",
        "  46: banana\n",
        "  47: apple\n",
        "  48: sandwich\n",
        "  49: orange\n",
        "  50: broccoli\n",
        "  51: carrot\n",
        "  52: hot dog\n",
        "  53: pizza\n",
        "  54: donut\n",
        "  55: cake\n",
        "  56: chair\n",
        "  57: couch\n",
        "  58: potted plant\n",
        "  59: bed\n",
        "  60: dining table\n",
        "  61: toilet\n",
        "  62: tv\n",
        "  63: laptop\n",
        "  64: mouse\n",
        "  65: remote\n",
        "  66: keyboard\n",
        "  67: cell phone\n",
        "  68: microwave\n",
        "  69: oven\n",
        "  70: toaster\n",
        "  71: sink\n",
        "  72: refrigerator\n",
        "  73: book\n",
        "  74: clock\n",
        "  75: vase\n",
        "  76: scissors\n",
        "  77: teddy bear\n",
        "  78: hair drier\n",
        "  79: toothbrush\n",
        "\n"
      ],
      "metadata": {
        "id": "0-2hDyhK3OGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0aa2ca-1d86-46b4-d282-106355c090e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/args.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dataset"
      ],
      "metadata": {
        "id": "9F85CnjT45Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/dataset.py\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "# Corrected resample function\n",
        "def resample():\n",
        "    return random.choice([cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC])\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import numpy\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "\n",
        "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
        "\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, filenames, input_size, params, augment):\n",
        "        self.params = params\n",
        "        self.mosaic = augment\n",
        "        self.augment = augment\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Read labels\n",
        "        cache = self.load_label(filenames)\n",
        "        labels, shapes = zip(*cache.values())\n",
        "        self.labels = list(labels)\n",
        "        self.shapes = numpy.array(shapes, dtype=numpy.float64)\n",
        "        self.filenames = list(cache.keys())  # update\n",
        "        self.n = len(shapes)  # number of samples\n",
        "        self.indices = range(self.n)\n",
        "        # Albumentations (optional, only used if package is installed)\n",
        "        self.albumentations = Albumentations()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]\n",
        "\n",
        "        params = self.params\n",
        "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
        "\n",
        "        if mosaic:\n",
        "            shapes = None\n",
        "            # Load MOSAIC\n",
        "            image, label = self.load_mosaic(index, params)\n",
        "            # MixUp augmentation\n",
        "            if random.random() < params['mix_up']:\n",
        "                index = random.choice(self.indices)\n",
        "                mix_image1, mix_label1 = image, label\n",
        "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
        "\n",
        "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
        "        else:\n",
        "            # Load image\n",
        "            image, shape = self.load_image(index)\n",
        "            h, w = image.shape[:2]\n",
        "\n",
        "            # Resize\n",
        "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
        "            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # for COCO mAP rescaling\n",
        "\n",
        "            label = self.labels[index].copy()\n",
        "            if label.size:\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
        "            if self.augment:\n",
        "                image, label = random_perspective(image, label, params)\n",
        "        nl = len(label)  # number of labels\n",
        "        if nl:\n",
        "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
        "\n",
        "        if self.augment:\n",
        "            # Albumentations\n",
        "            image, label = self.albumentations(image, label)\n",
        "            nl = len(label)  # update after albumentations\n",
        "            # HSV color-space\n",
        "            augment_hsv(image, params)\n",
        "            # Flip up-down\n",
        "            if random.random() < params['flip_ud']:\n",
        "                image = numpy.flipud(image)\n",
        "                if nl:\n",
        "                    label[:, 2] = 1 - label[:, 2]\n",
        "            # Flip left-right\n",
        "            if random.random() < params['flip_lr']:\n",
        "                image = numpy.fliplr(image)\n",
        "                if nl:\n",
        "                    label[:, 1] = 1 - label[:, 1]\n",
        "\n",
        "        target = torch.zeros((nl, 6))\n",
        "        if nl:\n",
        "            target[:, 1:] = torch.from_numpy(label)\n",
        "\n",
        "        # Convert HWC to CHW, BGR to RGB\n",
        "        sample = image.transpose((2, 0, 1))[::-1]\n",
        "        sample = numpy.ascontiguousarray(sample)\n",
        "\n",
        "        return torch.from_numpy(sample), target, shapes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def load_image(self, i):\n",
        "        image = cv2.imread(self.filenames[i])\n",
        "        h, w = image.shape[:2]\n",
        "        r = self.input_size / max(h, w)\n",
        "        if r != 1:\n",
        "            image = cv2.resize(image,\n",
        "                               dsize=(int(w * r), int(h * r)),\n",
        "                               interpolation=resample() if self.augment else resample())\n",
        "        return image, (h, w)\n",
        "\n",
        "    def load_mosaic(self, index, params):\n",
        "        label4 = []\n",
        "        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n",
        "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
        "\n",
        "        border = [-self.input_size // 2, -self.input_size // 2]\n",
        "\n",
        "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "\n",
        "        indices = [index] + random.choices(self.indices, k=3)\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        for i, index in enumerate(indices):\n",
        "            # Load image\n",
        "            image, _ = self.load_image(index)\n",
        "            shape = image.shape\n",
        "            if i == 0:  # top left\n",
        "                x1a = max(xc - shape[1], 0)\n",
        "                y1a = max(yc - shape[0], 0)\n",
        "                x2a = xc\n",
        "                y2a = yc\n",
        "                x1b = shape[1] - (x2a - x1a)\n",
        "                y1b = shape[0] - (y2a - y1a)\n",
        "                x2b = shape[1]\n",
        "                y2b = shape[0]\n",
        "            if i == 1:  # top right\n",
        "                x1a = xc\n",
        "                y1a = max(yc - shape[0], 0)\n",
        "                x2a = min(xc + shape[1], self.input_size * 2)\n",
        "                y2a = yc\n",
        "                x1b = 0\n",
        "                y1b = shape[0] - (y2a - y1a)\n",
        "                x2b = min(shape[1], x2a - x1a)\n",
        "                y2b = shape[0]\n",
        "            if i == 2:  # bottom left\n",
        "                x1a = max(xc - shape[1], 0)\n",
        "                y1a = yc\n",
        "                x2a = xc\n",
        "                y2a = min(self.input_size * 2, yc + shape[0])\n",
        "                x1b = shape[1] - (x2a - x1a)\n",
        "                y1b = 0\n",
        "                x2b = shape[1]\n",
        "                y2b = min(y2a - y1a, shape[0])\n",
        "            if i == 3:  # bottom right\n",
        "                x1a = xc\n",
        "                y1a = yc\n",
        "                x2a = min(xc + shape[1], self.input_size * 2)\n",
        "                y2a = min(self.input_size * 2, yc + shape[0])\n",
        "                x1b = 0\n",
        "                y1b = 0\n",
        "                x2b = min(shape[1], x2a - x1a)\n",
        "                y2b = min(y2a - y1a, shape[0])\n",
        "\n",
        "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
        "            pad_w = x1a - x1b\n",
        "            pad_h = y1a - y1b\n",
        "\n",
        "            # Labels\n",
        "            label = self.labels[index].copy()\n",
        "            if len(label):\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
        "            label4.append(label)\n",
        "\n",
        "        # Concat/clip labels\n",
        "        label4 = numpy.concatenate(label4, 0)\n",
        "        for x in label4[:, 1:]:\n",
        "            numpy.clip(x, 0, 2 * self.input_size, out=x)\n",
        "\n",
        "        # Augment\n",
        "        image4, label4 = random_perspective(image4, label4, params, border)\n",
        "\n",
        "        return image4, label4\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        samples, targets, shapes = zip(*batch)\n",
        "        for i, item in enumerate(targets):\n",
        "            item[:, 0] = i  # add target image index\n",
        "        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def load_label(filenames):\n",
        "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
        "        if os.path.exists(path):\n",
        "            return torch.load(path,weights_only=False)\n",
        "        x = {}\n",
        "        for filename in filenames:\n",
        "            try:\n",
        "                # verify images\n",
        "                with open(filename, 'rb') as f:\n",
        "                    image = Image.open(f)\n",
        "                    image.verify()  # PIL verify\n",
        "                shape = image.size  # image size\n",
        "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
        "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
        "\n",
        "                # verify labels\n",
        "                a = f'{os.sep}images{os.sep}'\n",
        "                b = f'{os.sep}labels{os.sep}'\n",
        "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
        "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
        "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
        "                        label = numpy.array(label, dtype=numpy.float32)\n",
        "                    nl = len(label)\n",
        "                    if nl:\n",
        "                        assert label.shape[1] == 5, 'labels require 5 columns'\n",
        "                        assert (label >= 0).all(), 'negative label values'\n",
        "                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n",
        "                        _, i = numpy.unique(label, axis=0, return_index=True)\n",
        "                        if len(i) < nl:  # duplicate row check\n",
        "                            label = label[i]  # remove duplicates\n",
        "                    else:\n",
        "                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
        "                else:\n",
        "                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
        "                if filename:\n",
        "                    x[filename] = [label, shape]\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "        torch.save(x, path)\n",
        "        return x\n",
        "\n",
        "\n",
        "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
        "    # Convert nx4 boxes\n",
        "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = numpy.copy(x)\n",
        "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
        "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
        "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
        "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xy2wh(x, w=640, h=640):\n",
        "    # warning: inplace clip\n",
        "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
        "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
        "    y = numpy.copy(x)\n",
        "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center normalized\n",
        "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center normalized\n",
        "    y[:, 2] = (x[:, 2] - x[:, 0]) / w        # width normalized\n",
        "    y[:, 3] = (x[:, 3] - x[:, 1]) / h        # height normalized\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "def resample():\n",
        "    # Return one random interpolation method from the list\n",
        "    return random.choice([cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC])\n",
        "\n",
        "\n",
        "\n",
        "def augment_hsv(image, params):\n",
        "    # HSV color-space augmentation\n",
        "    h = params['hsv_h']\n",
        "    s = params['hsv_s']\n",
        "    v = params['hsv_v']\n",
        "\n",
        "    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
        "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
        "\n",
        "    x = numpy.arange(0, 256, dtype=r.dtype)\n",
        "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
        "    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n",
        "    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n",
        "\n",
        "    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
        "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
        "\n",
        "\n",
        "def resize(image, input_size, augment):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = image.shape[:2]  # current shape [height, width]\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(input_size / shape[0], input_size / shape[1])\n",
        "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    w = (input_size - pad[0]) / 2\n",
        "    h = (input_size - pad[1]) / 2\n",
        "\n",
        "    if shape[::-1] != pad:  # resize\n",
        "        image = cv2.resize(image,\n",
        "                           dsize=pad,\n",
        "                           interpolation=resample() if augment else resample())\n",
        "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
        "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
        "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
        "    return image, (r, r), (w, h)\n",
        "\n",
        "\n",
        "def candidates(box1, box2):\n",
        "    # box1(4,n), box2(4,n)\n",
        "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
        "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
        "\n",
        "\n",
        "def random_perspective(samples, targets, params, border=(0, 0)):\n",
        "    h = samples.shape[0] + border[0] * 2\n",
        "    w = samples.shape[1] + border[1] * 2\n",
        "\n",
        "    # Center\n",
        "    center = numpy.eye(3)\n",
        "    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n",
        "    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n",
        "\n",
        "    # Perspective\n",
        "    perspective = numpy.eye(3)\n",
        "\n",
        "    # Rotation and Scale\n",
        "    rotate = numpy.eye(3)\n",
        "    a = random.uniform(-params['degrees'], params['degrees'])\n",
        "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
        "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "\n",
        "    # Shear\n",
        "    shear = numpy.eye(3)\n",
        "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "\n",
        "    # Translation\n",
        "    translate = numpy.eye(3)\n",
        "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
        "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
        "\n",
        "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
        "    matrix = translate @ shear @ rotate @ perspective @ center\n",
        "    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n",
        "        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
        "\n",
        "    # Transform label coordinates\n",
        "    n = len(targets)\n",
        "    if n:\n",
        "        xy = numpy.ones((n * 4, 3))\n",
        "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
        "        xy = xy @ matrix.T  # transform\n",
        "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
        "\n",
        "        # create new boxes\n",
        "        x = xy[:, [0, 2, 4, 6]]\n",
        "        y = xy[:, [1, 3, 5, 7]]\n",
        "        new = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "\n",
        "        # clip\n",
        "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n",
        "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n",
        "\n",
        "        # filter candidates\n",
        "        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n",
        "        targets = targets[indices]\n",
        "        targets[:, 1:5] = new[indices]\n",
        "\n",
        "    return samples, targets\n",
        "\n",
        "\n",
        "def mix_up(image1, label1, image2, label2):\n",
        "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
        "    alpha = numpy.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n",
        "    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n",
        "    label = numpy.concatenate((label1, label2), 0)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "class Albumentations:\n",
        "    def __init__(self):\n",
        "        self.transform = None\n",
        "        try:\n",
        "            import albumentations as album\n",
        "\n",
        "            transforms = [album.Blur(p=0.01),\n",
        "                          album.CLAHE(p=0.01),\n",
        "                          album.ToGray(p=0.01),\n",
        "                          album.MedianBlur(p=0.01)]\n",
        "            self.transform = album.Compose(transforms,\n",
        "                                           album.BboxParams('yolo', ['class_labels']))\n",
        "\n",
        "        except ImportError:  # package not installed, skip\n",
        "            pass\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "        if self.transform:\n",
        "            x = self.transform(image=image,\n",
        "                               bboxes=label[:, 1:],\n",
        "                               class_labels=label[:, 0])\n",
        "            image = x['image']\n",
        "            label = numpy.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "O7Jo-w4HIuHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606d346b-4995-4749-f3a5-450f3057a151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "iIFZ-iadQYt4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqPhJKqEyHsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##util"
      ],
      "metadata": {
        "id": "tWbLRgbarSLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/util.py\n",
        "\n",
        "\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.nn.functional import cross_entropy, one_hot\n",
        "\n",
        "\n",
        "def setup_seed():\n",
        "    \"\"\"\n",
        "    Setup random seed.\n",
        "    \"\"\"\n",
        "    random.seed(0)\n",
        "    numpy.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def setup_multi_processes():\n",
        "    \"\"\"\n",
        "    Setup multi-processing environment variables.\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    from os import environ\n",
        "    from platform import system\n",
        "\n",
        "    # set multiprocess start method as `fork` to speed up the training\n",
        "    if system() != 'Windows':\n",
        "        torch.multiprocessing.set_start_method('fork', force=True)\n",
        "\n",
        "    # disable opencv multithreading to avoid system being overloaded\n",
        "    cv2.setNumThreads(0)\n",
        "\n",
        "    # setup OMP threads\n",
        "    if 'OMP_NUM_THREADS' not in environ:\n",
        "        environ['OMP_NUM_THREADS'] = '1'\n",
        "\n",
        "    # setup MKL threads\n",
        "    if 'MKL_NUM_THREADS' not in environ:\n",
        "        environ['MKL_NUM_THREADS'] = '1'\n",
        "\n",
        "\n",
        "def scale(coords, shape1, shape2, ratio_pad=None):\n",
        "    if ratio_pad is None:  # calculate from img0_shape\n",
        "        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n",
        "        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n",
        "    else:\n",
        "        gain = ratio_pad[0][0]\n",
        "        pad = ratio_pad[1]\n",
        "\n",
        "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
        "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
        "    coords[:, :4] /= gain\n",
        "\n",
        "    coords[:, 0].clamp_(0, shape2[1])  # x1\n",
        "    coords[:, 1].clamp_(0, shape2[0])  # y1\n",
        "    coords[:, 2].clamp_(0, shape2[1])  # x2\n",
        "    coords[:, 3].clamp_(0, shape2[0])  # y2\n",
        "    return coords\n",
        "\n",
        "\n",
        "def make_anchors(x, strides, offset=0.5):\n",
        "    \"\"\"\n",
        "    Generate anchors from features\n",
        "    \"\"\"\n",
        "    assert x is not None\n",
        "    anchor_points, stride_tensor = [], []\n",
        "    for i, stride in enumerate(strides):\n",
        "        _, _, h, w = x[i].shape\n",
        "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
        "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
        "        sy, sx = torch.meshgrid(sy, sx)\n",
        "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
        "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
        "\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
        "    \"\"\"\n",
        "    Return intersection-over-union (Jaccard index) of boxes.\n",
        "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "    Arguments:\n",
        "        box1 (Tensor[N, 4])\n",
        "        box2 (Tensor[M, 4])\n",
        "    Returns:\n",
        "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "            IoU values for every element in boxes1 and boxes2\n",
        "    \"\"\"\n",
        "\n",
        "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
        "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
        "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
        "\n",
        "    # IoU = intersection / (area1 + area2 - intersection)\n",
        "    box1 = box1.T\n",
        "    box2 = box2.T\n",
        "\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    return intersection / (area1[:, None] + area2 - intersection)\n",
        "\n",
        "\n",
        "def wh2xy(x):\n",
        "    y = x.clone()\n",
        "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
        "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
        "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
        "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
        "    nc = prediction.shape[1] - 4  # number of classes\n",
        "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n",
        "\n",
        "    # Settings\n",
        "    max_wh = 7680  # (pixels) maximum box width and height\n",
        "    max_det = 300  # the maximum number of boxes to keep after NMS\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "\n",
        "    start = time.time()\n",
        "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "    for index, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Detections matrix nx6 (box, conf, cls)\n",
        "        box, cls = x.split((4, nc), 1)\n",
        "        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
        "        box = wh2xy(box)\n",
        "        if nc > 1:\n",
        "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = cls.max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
        "        # Check shape\n",
        "        if not x.shape[0]:  # no boxes\n",
        "            continue\n",
        "        # sort by confidence and remove excess boxes\n",
        "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * max_wh  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
        "        i = i[:max_det]  # limit detections\n",
        "        outputs[index] = x[i]\n",
        "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
        "            print(f'WARNING ‚ö†Ô∏è NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n",
        "            break  # time limit exceeded\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def smooth(y, f=0.05):\n",
        "    # Box filter of fraction f\n",
        "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
        "    p = numpy.ones(nf // 2)  # ones padding\n",
        "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
        "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
        "\n",
        "\n",
        "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
        "    \"\"\"\n",
        "    Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:  True positives (nparray, nx1 or nx10).\n",
        "        conf:  Object-ness value from 0-1 (nparray).\n",
        "        pred_cls:  Predicted object classes (nparray).\n",
        "        target_cls:  True object classes (nparray).\n",
        "    # Returns\n",
        "        The average precision\n",
        "    \"\"\"\n",
        "    # Sort by object-ness\n",
        "    i = numpy.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n",
        "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    p = numpy.zeros((nc, 1000))\n",
        "    r = numpy.zeros((nc, 1000))\n",
        "    ap = numpy.zeros((nc, tp.shape[1]))\n",
        "    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n",
        "    for ci, c in enumerate(unique_classes):\n",
        "        i = pred_cls == c\n",
        "        nl = nt[ci]  # number of labels\n",
        "        no = i.sum()  # number of outputs\n",
        "        if no == 0 or nl == 0:\n",
        "            continue\n",
        "\n",
        "        # Accumulate FPs and TPs\n",
        "        fpc = (1 - tp[i]).cumsum(0)\n",
        "        tpc = tp[i].cumsum(0)\n",
        "\n",
        "        # Recall\n",
        "        recall = tpc / (nl + eps)  # recall curve\n",
        "        # negative x, xp because xp decreases\n",
        "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
        "\n",
        "        # Precision\n",
        "        precision = tpc / (tpc + fpc)  # precision curve\n",
        "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
        "\n",
        "        # AP from recall-precision curve\n",
        "        for j in range(tp.shape[1]):\n",
        "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
        "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
        "\n",
        "            # Compute the precision envelope\n",
        "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
        "\n",
        "            # Integrate area under curve\n",
        "            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
        "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
        "\n",
        "    # Compute F1 (harmonic mean of precision and recall)\n",
        "    f1 = 2 * p * r / (p + r + eps)\n",
        "\n",
        "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
        "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
        "    tp = (r * nt).round()  # true positives\n",
        "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
        "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "    m_pre, m_rec = p.mean(), r.mean()\n",
        "    map50, mean_ap = ap50.mean(), ap.mean()\n",
        "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
        "\n",
        "\n",
        "def strip_optimizer(filename):\n",
        "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
        "    x['model'].half()  # to FP16\n",
        "    for p in x['model'].parameters():\n",
        "        p.requires_grad = False\n",
        "    torch.save(x, filename)\n",
        "\n",
        "\n",
        "def clip_gradients(model, max_norm=10.0):\n",
        "    parameters = model.parameters()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    \"\"\"\n",
        "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
        "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
        "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
        "        # Create EMA\n",
        "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
        "        self.updates = updates  # number of EMA updates\n",
        "        # decay exponential ramp (to help early epochs)\n",
        "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def update(self, model):\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "        # Update EMA parameters\n",
        "        with torch.no_grad():\n",
        "            self.updates += 1\n",
        "            d = self.decay(self.updates)\n",
        "\n",
        "            msd = model.state_dict()  # model state_dict\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= d\n",
        "                    v += (1 - d) * msd[k].detach()\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.num = 0\n",
        "        self.sum = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, v, n):\n",
        "        if not math.isnan(float(v)):\n",
        "            self.num = self.num + n\n",
        "            self.sum = self.sum + v * n\n",
        "            self.avg = self.sum / self.num\n",
        "\n",
        "\n",
        "class ComputeLoss:\n",
        "    def __init__(self, model, params):\n",
        "        super().__init__()\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "\n",
        "        m = model.head  # Head() module\n",
        "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.stride = m.stride  # model strides\n",
        "        self.nc = m.nc  # number of classes\n",
        "        self.no = m.no\n",
        "        self.device = device\n",
        "        self.params = params\n",
        "\n",
        "        # task aligned assigner\n",
        "        self.top_k = 10\n",
        "        self.alpha = 0.5\n",
        "        self.beta = 6.0\n",
        "        self.eps = 1e-9\n",
        "\n",
        "        self.bs = 1\n",
        "        self.num_max_boxes = 0\n",
        "        # DFL Loss params\n",
        "        self.dfl_ch = m.dfl.ch\n",
        "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
        "\n",
        "    def __call__(self, outputs, targets):\n",
        "        x = outputs[1] if isinstance(outputs, tuple) else outputs\n",
        "        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
        "        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n",
        "\n",
        "        pred_output = pred_output.permute(0, 2, 1).contiguous()\n",
        "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
        "\n",
        "        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
        "        size = size * self.stride[0]\n",
        "\n",
        "        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n",
        "\n",
        "        # targets\n",
        "        if targets.shape[0] == 0:\n",
        "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
        "        else:\n",
        "            i = targets[:, 0]  # image index\n",
        "            _, counts = i.unique(return_counts=True)\n",
        "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
        "            for j in range(pred_scores.shape[0]):\n",
        "                matches = i == j\n",
        "                n = matches.sum()\n",
        "                if n:\n",
        "                    gt[j, :n] = targets[matches, 1:]\n",
        "            gt[..., 1:5] = wh2xy(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
        "\n",
        "        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n",
        "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
        "\n",
        "        # boxes\n",
        "        b, a, c = pred_output.shape\n",
        "        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n",
        "        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n",
        "\n",
        "        a, b = torch.split(pred_bboxes, 2, -1)\n",
        "        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n",
        "\n",
        "        scores = pred_scores.detach().sigmoid()\n",
        "        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
        "        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n",
        "                                                            gt_labels, gt_bboxes, mask_gt,\n",
        "                                                            anchor_points * stride_tensor)\n",
        "\n",
        "        target_bboxes /= stride_tensor\n",
        "        target_scores_sum = target_scores.sum()\n",
        "\n",
        "        # cls loss\n",
        "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
        "        loss_cls = loss_cls.sum() / target_scores_sum\n",
        "\n",
        "        # box loss\n",
        "        loss_box = torch.zeros(1, device=self.device)\n",
        "        loss_dfl = torch.zeros(1, device=self.device)\n",
        "        if fg_mask.sum():\n",
        "            # IoU loss\n",
        "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
        "            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
        "            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n",
        "            # DFL loss\n",
        "            a, b = torch.split(target_bboxes, 2, -1)\n",
        "            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
        "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n",
        "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n",
        "            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
        "\n",
        "        loss_cls *= self.params['cls']\n",
        "        loss_box *= self.params['box']\n",
        "        loss_dfl *= self.params['dfl']\n",
        "        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
        "        \"\"\"\n",
        "        Task-aligned One-stage Object Detection assigner\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Sanitize the true_labels to ensure no invalid indices\n",
        "        true_labels = torch.clamp(true_labels, min=0, max=self.nc - 1)  # Clip to [0, num_classes-1]\n",
        "\n",
        "        self.bs = pred_scores.size(0)\n",
        "        self.num_max_boxes = true_bboxes.size(1)\n",
        "\n",
        "        if self.num_max_boxes == 0:\n",
        "            device = true_bboxes.device\n",
        "            return (torch.full_like(pred_scores[..., 0], self.nc).to(device),\n",
        "                    torch.zeros_like(pred_bboxes).to(device),\n",
        "                    torch.zeros_like(pred_scores).to(device),\n",
        "                    torch.zeros_like(pred_scores[..., 0]).to(device),\n",
        "                    torch.zeros_like(pred_scores[..., 0]).to(device))\n",
        "\n",
        "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long, device=true_labels.device)\n",
        "        i[0] = torch.arange(end=self.bs, device=true_labels.device).view(-1, 1).repeat(1, self.num_max_boxes)\n",
        "        i[1] = true_labels.long().squeeze(-1)\n",
        "\n",
        "        # üß† Safety check: ensure all label indices are within valid class range\n",
        "        num_classes = pred_scores.shape[2]\n",
        "        if not ((i[1] >= 0) & (i[1] < num_classes)).all():\n",
        "            raise ValueError(\n",
        "                f\"[assign()] Invalid class indices in true_labels! \"\n",
        "                f\"Found min={i[1].min().item()}, max={i[1].max().item()}, but num_classes={num_classes}\"\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n",
        "        overlaps = overlaps.squeeze(3).clamp(0)\n",
        "        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n",
        "        bs, n_boxes, _ = true_bboxes.shape\n",
        "        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
        "        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n",
        "        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n",
        "        metrics = align_metric * mask_in_gts\n",
        "        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n",
        "        num_anchors = metrics.shape[-1]\n",
        "        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n",
        "        if top_k_mask is None:\n",
        "            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n",
        "        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n",
        "        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n",
        "        # filter invalid boxes\n",
        "        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n",
        "        mask_top_k = is_in_top_k.to(metrics.dtype)\n",
        "        # merge all mask to a final mask, (b, max_num_obj, h*w)\n",
        "        mask_pos = mask_top_k * mask_in_gts * true_mask\n",
        "\n",
        "        fg_mask = mask_pos.sum(-2)\n",
        "        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n",
        "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n",
        "            max_overlaps_idx = overlaps.argmax(1)\n",
        "            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n",
        "            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
        "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
        "            fg_mask = mask_pos.sum(-2)\n",
        "        # find each grid serve which gt(index)\n",
        "        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
        "\n",
        "        # assigned target labels, (b, 1)\n",
        "        batch_index = torch.arange(end=self.bs,\n",
        "                                   dtype=torch.int64,\n",
        "                                   device=true_labels.device)[..., None]\n",
        "        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n",
        "        target_labels = true_labels.long().flatten()[target_gt_idx]\n",
        "\n",
        "        # assigned target boxes\n",
        "        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n",
        "\n",
        "        # assigned target scores\n",
        "        target_labels.clamp(0)\n",
        "        target_scores = one_hot(target_labels, self.nc)\n",
        "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
        "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
        "\n",
        "        # normalize\n",
        "        align_metric *= mask_pos\n",
        "        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
        "        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
        "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n",
        "        norm_align_metric = norm_align_metric.unsqueeze(-1)\n",
        "        target_scores = target_scores * norm_align_metric\n",
        "\n",
        "        return target_bboxes, target_scores, fg_mask.bool()\n",
        "\n",
        "    @staticmethod\n",
        "    def df_loss(pred_dist, target):\n",
        "        # Return sum of left and right DFL losses\n",
        "        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
        "        tl = target.long()  # target left\n",
        "        tr = tl + 1  # target right\n",
        "        wl = tr - target  # weight left\n",
        "        wr = 1 - wl  # weight right\n",
        "        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n",
        "        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n",
        "        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def iou(box1, box2, eps=1e-7):\n",
        "        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
        "\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
        "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
        "\n",
        "        # Intersection area\n",
        "        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n",
        "        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
        "        intersection = area1.clamp(0) * area2.clamp(0)\n",
        "\n",
        "        # Union Area\n",
        "        union = w1 * h1 + w2 * h2 - intersection + eps\n",
        "\n",
        "        # IoU\n",
        "        iou = intersection / union\n",
        "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n",
        "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
        "        # Complete IoU https://arxiv.org/abs/1911.08287v1\n",
        "        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
        "        # center dist ** 2\n",
        "        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
        "        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
        "        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
        "        with torch.no_grad():\n",
        "            alpha = v / (v - iou + (1 + eps))\n",
        "        return iou - (rho2 / c2 + v * alpha)  # CIoU"
      ],
      "metadata": {
        "id": "izCZhvAKC92F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34801ade-0954-4cc6-ab6f-37b003caa95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#nets"
      ],
      "metadata": {
        "id": "en5AbGCXFpcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(\"nets\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "YknSrS8JwUCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nets/nn.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from utils.util import make_anchors\n",
        "\n",
        "\n",
        "def pad(k, p=None, d=1):\n",
        "    if d > 1:\n",
        "        k = d * (k - 1) + 1\n",
        "    if p is None:\n",
        "        p = k // 2\n",
        "    return p\n",
        "\n",
        "\n",
        "def fuse_conv(conv, norm):\n",
        "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
        "                                 conv.out_channels,\n",
        "                                 kernel_size=conv.kernel_size,\n",
        "                                 stride=conv.stride,\n",
        "                                 padding=conv.padding,\n",
        "                                 groups=conv.groups,\n",
        "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
        "\n",
        "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
        "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
        "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
        "\n",
        "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
        "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
        "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
        "\n",
        "    return fused_conv\n",
        "\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n",
        "        super().__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n",
        "        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n",
        "        self.relu = torch.nn.SiLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.norm(self.conv(x)))\n",
        "\n",
        "    def fuse_forward(self, x):\n",
        "        return self.relu(self.conv(x))\n",
        "\n",
        "\n",
        "class Residual(torch.nn.Module):\n",
        "    def __init__(self, ch, add=True):\n",
        "        super().__init__()\n",
        "        self.add_m = add\n",
        "        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n",
        "                                         Conv(ch, ch, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.res_m(x) + x if self.add_m else self.res_m(x)\n",
        "\n",
        "\n",
        "class CSP(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, n=1, add=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_ch, out_ch // 2)\n",
        "        self.conv2 = Conv(in_ch, out_ch // 2)\n",
        "        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n",
        "        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = [self.conv1(x), self.conv2(x)]\n",
        "        y.extend(m(y[-1]) for m in self.res_m)\n",
        "        return self.conv3(torch.cat(y, dim=1))\n",
        "\n",
        "\n",
        "class SPP(torch.nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=5):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(in_ch, in_ch // 2)\n",
        "        self.conv2 = Conv(in_ch * 2, out_ch)\n",
        "        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        y1 = self.res_m(x)\n",
        "        y2 = self.res_m(y1)\n",
        "        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n",
        "\n",
        "\n",
        "class DarkNet(torch.nn.Module):\n",
        "    def __init__(self, width, depth):\n",
        "        super().__init__()\n",
        "        p1 = [Conv(width[0], width[1], 3, 2)]\n",
        "        p2 = [Conv(width[1], width[2], 3, 2),\n",
        "              CSP(width[2], width[2], depth[0])]\n",
        "        p3 = [Conv(width[2], width[3], 3, 2),\n",
        "              CSP(width[3], width[3], depth[1])]\n",
        "        p4 = [Conv(width[3], width[4], 3, 2),\n",
        "              CSP(width[4], width[4], depth[2])]\n",
        "        p5 = [Conv(width[4], width[5], 3, 2),\n",
        "              CSP(width[5], width[5], depth[0]),\n",
        "              SPP(width[5], width[5])]\n",
        "\n",
        "        self.p1 = torch.nn.Sequential(*p1)\n",
        "        self.p2 = torch.nn.Sequential(*p2)\n",
        "        self.p3 = torch.nn.Sequential(*p3)\n",
        "        self.p4 = torch.nn.Sequential(*p4)\n",
        "        self.p5 = torch.nn.Sequential(*p5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p1 = self.p1(x)\n",
        "        p2 = self.p2(p1)\n",
        "        p3 = self.p3(p2)\n",
        "        p4 = self.p4(p3)\n",
        "        p5 = self.p5(p4)\n",
        "        return p3, p4, p5\n",
        "\n",
        "\n",
        "class DarkFPN(torch.nn.Module):\n",
        "    def __init__(self, width, depth):\n",
        "        super().__init__()\n",
        "        self.up = torch.nn.Upsample(None, 2)\n",
        "        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n",
        "        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n",
        "        self.h3 = Conv(width[3], width[3], 3, 2)\n",
        "        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n",
        "        self.h5 = Conv(width[4], width[4], 3, 2)\n",
        "        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p3, p4, p5 = x\n",
        "        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n",
        "        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n",
        "        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n",
        "        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n",
        "        return h2, h4, h6\n",
        "\n",
        "\n",
        "class DFL(torch.nn.Module):\n",
        "    # Integral module of Distribution Focal Loss (DFL)\n",
        "    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
        "    def __init__(self, ch=16):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n",
        "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
        "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, a = x.shape\n",
        "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
        "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
        "\n",
        "\n",
        "class Head(torch.nn.Module):\n",
        "    anchors = torch.empty(0)\n",
        "    strides = torch.empty(0)\n",
        "\n",
        "    def __init__(self, nc=80, filters=()):\n",
        "        super().__init__()\n",
        "        self.ch = 16  # DFL channels\n",
        "        self.nc = nc  # number of classes\n",
        "        self.nl = len(filters)  # number of detection layers\n",
        "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
        "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
        "\n",
        "        c1 = max(filters[0], self.nc)\n",
        "        c2 = max((filters[0] // 4, self.ch * 4))\n",
        "\n",
        "        self.dfl = DFL(self.ch)\n",
        "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n",
        "                                                           Conv(c1, c1, 3),\n",
        "                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n",
        "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n",
        "                                                           Conv(c2, c2, 3),\n",
        "                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.nl):\n",
        "            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n",
        "        if self.training:\n",
        "            return x\n",
        "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
        "\n",
        "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
        "\n",
        "        box, cls = x.split((self.ch * 4, self.nc), 1)\n",
        "        a, b = torch.split(self.dfl(box), 2, 1)\n",
        "        a = self.anchors.unsqueeze(0) - a\n",
        "        b = self.anchors.unsqueeze(0) + b\n",
        "        box = torch.cat(((a + b) / 2, b - a), 1)\n",
        "        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n",
        "\n",
        "    def initialize_biases(self):\n",
        "        # Initialize biases\n",
        "        # WARNING: requires stride availability\n",
        "        m = self\n",
        "        for a, b, s in zip(m.box, m.cls, m.stride):\n",
        "            a[-1].bias.data[:] = 1.0  # box\n",
        "            # cls (.01 objects, 80 classes, 640 img)\n",
        "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
        "\n",
        "\n",
        "class YOLO(torch.nn.Module):\n",
        "    def __init__(self, width, depth, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = DarkNet(width, depth)\n",
        "        self.fpn = DarkFPN(width, depth)\n",
        "\n",
        "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
        "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
        "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
        "        self.stride = self.head.stride\n",
        "        self.head.initialize_biases()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        x = self.fpn(x)\n",
        "        return self.head(list(x))\n",
        "\n",
        "    def fuse(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) is Conv and hasattr(m, 'norm'):\n",
        "                m.conv = fuse_conv(m.conv, m.norm)\n",
        "                m.forward = m.fuse_forward\n",
        "                delattr(m, 'norm')\n",
        "        return self\n",
        "\n",
        "\n",
        "def yolo_v8_n(num_classes: int = 80):\n",
        "    depth = [1, 2, 2]\n",
        "    width = [3, 16, 32, 64, 128, 256]\n",
        "    return YOLO(width, depth, num_classes)\n",
        "\n",
        "\n",
        "def yolo_v8_s(num_classes: int = 80):\n",
        "    depth = [1, 2, 2]\n",
        "    width = [3, 32, 64, 128, 256, 512]\n",
        "    return YOLO(width, depth, num_classes)\n",
        "\n",
        "\n",
        "def yolo_v8_m(num_classes: int = 80):\n",
        "    depth = [2, 4, 4]\n",
        "    width = [3, 48, 96, 192, 384, 576]\n",
        "    return YOLO(width, depth, num_classes)\n",
        "\n",
        "\n",
        "def yolo_v8_l(num_classes: int = 80):\n",
        "    depth = [3, 6, 6]\n",
        "    width = [3, 64, 128, 256, 512, 512]\n",
        "    return YOLO(width, depth, num_classes)\n",
        "\n",
        "\n",
        "def yolo_v8_x(num_classes: int = 80):\n",
        "    depth = [3, 6, 6]\n",
        "    width = [3, 80, 160, 320, 640, 640]\n",
        "    return YOLO(width, depth, num_classes)"
      ],
      "metadata": {
        "id": "Ppv6-iXzFh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea571c4-7076-43eb-fe02-82d6de08951c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nets/nn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Jw3GNNH5Fh3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G19PZe_1tOgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main"
      ],
      "metadata": {
        "id": "u6vWT3kuIunE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  main.py\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import tqdm\n",
        "import yaml\n",
        "from torch.utils import data\n",
        "\n",
        "from nets import nn\n",
        "from utils import util\n",
        "from utils.dataset import Dataset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def learning_rate(args, params):\n",
        "    def fn(x):\n",
        "        return (1 - x / args.epochs) * (1.0 - params['lrf']) + params['lrf']\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "def train(args, params):\n",
        "    # Model\n",
        "    model = nn.yolo_v8_n(len(params['names'].values())).cuda()\n",
        "\n",
        "    # Optimizer\n",
        "    accumulate = max(round(64 / (args.batch_size * args.world_size)), 1)\n",
        "    params['weight_decay'] *= args.batch_size * args.world_size * accumulate / 64\n",
        "\n",
        "    p = [], [], []\n",
        "    for v in model.modules():\n",
        "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
        "            p[2].append(v.bias)\n",
        "        if isinstance(v, torch.nn.BatchNorm2d):\n",
        "            p[1].append(v.weight)\n",
        "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
        "            p[0].append(v.weight)\n",
        "\n",
        "    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n",
        "\n",
        "    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n",
        "    optimizer.add_param_group({'params': p[1]})\n",
        "    del p\n",
        "\n",
        "    # Scheduler\n",
        "    lr = learning_rate(args, params)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr, last_epoch=-1)\n",
        "\n",
        "    # EMA\n",
        "    ema = util.EMA(model) if args.local_rank == 0 else None\n",
        "\n",
        "    filenames = []\n",
        "    with open('/content/train2017.txt') as reader:\n",
        "        for filename in reader.readlines():\n",
        "            filename = filename.rstrip().split('/')[-1]\n",
        "            filenames.append('/content/COCO/images/train2017/' + filename)\n",
        "\n",
        "    dataset = Dataset(filenames, args.input_size, params, True)\n",
        "\n",
        "    if args.world_size <= 1:\n",
        "        sampler = None\n",
        "    else:\n",
        "        sampler = data.distributed.DistributedSampler(dataset)\n",
        "\n",
        "    loader = data.DataLoader(dataset, args.batch_size, sampler is None, sampler,\n",
        "                             num_workers=8, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
        "\n",
        "    if args.world_size > 1:\n",
        "        # DDP mode\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "        model = torch.nn.parallel.DistributedDataParallel(module=model,\n",
        "                                                          device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank)\n",
        "\n",
        "    # Start training\n",
        "    best = 0\n",
        "    num_batch = len(loader)\n",
        "    amp_scale = torch.cuda.amp.GradScaler()\n",
        "    criterion = util.ComputeLoss(model, params)\n",
        "    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n",
        "\n",
        "    import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "    os.makedirs(\"weights\", exist_ok=True)\n",
        "\n",
        "    with open('weights/step.csv', 'w') as f:\n",
        "        if args.local_rank == 0:\n",
        "            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n",
        "            writer.writeheader()\n",
        "        for epoch in range(args.epochs):\n",
        "            model.train()\n",
        "\n",
        "            if args.epochs - epoch == 10:\n",
        "                loader.dataset.mosaic = False\n",
        "\n",
        "            m_loss = util.AverageMeter()\n",
        "            if args.world_size > 1:\n",
        "                sampler.set_epoch(epoch)\n",
        "            p_bar = enumerate(loader)\n",
        "            if args.local_rank == 0:\n",
        "                print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n",
        "            if args.local_rank == 0:\n",
        "                p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            for i, (samples, targets, _) in p_bar:\n",
        "                x = i + num_batch * epoch  # number of iterations\n",
        "                samples = samples.cuda().float() / 255\n",
        "                targets = targets.cuda()\n",
        "\n",
        "                # Warmup\n",
        "                if x <= num_warmup:\n",
        "                    xp = [0, num_warmup]\n",
        "                    fp = [1, 64 / (args.batch_size * args.world_size)]\n",
        "                    accumulate = max(1, numpy.interp(x, xp, fp).round())\n",
        "                    for j, y in enumerate(optimizer.param_groups):\n",
        "                        if j == 0:\n",
        "                            fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n",
        "                        else:\n",
        "                            fp = [0.0, y['initial_lr'] * lr(epoch)]\n",
        "                        y['lr'] = numpy.interp(x, xp, fp)\n",
        "                        if 'momentum' in y:\n",
        "                            fp = [params['warmup_momentum'], params['momentum']]\n",
        "                            y['momentum'] = numpy.interp(x, xp, fp)\n",
        "\n",
        "                # Forward\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(samples)  # forward\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                m_loss.update(loss.item(), samples.size(0))\n",
        "\n",
        "                loss *= args.batch_size  # loss scaled by batch_size\n",
        "                loss *= args.world_size  # gradient averaged between devices in DDP mode\n",
        "\n",
        "                # Backward\n",
        "                amp_scale.scale(loss).backward()\n",
        "\n",
        "                # Optimize\n",
        "                if x % accumulate == 0:\n",
        "                    amp_scale.unscale_(optimizer)  # unscale gradients\n",
        "                    util.clip_gradients(model)  # clip gradients\n",
        "                    amp_scale.step(optimizer)  # optimizer.step\n",
        "                    amp_scale.update()\n",
        "                    optimizer.zero_grad()\n",
        "                    if ema:\n",
        "                        ema.update(model)\n",
        "\n",
        "                # Log\n",
        "                if args.local_rank == 0:\n",
        "                    memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
        "                    s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args.epochs}', memory, m_loss.avg)\n",
        "                    p_bar.set_description(s)\n",
        "\n",
        "                del loss\n",
        "                del outputs\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            if args.local_rank == 0:\n",
        "                # mAP\n",
        "                last = test(args, params, ema.ema)\n",
        "                writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n",
        "                                 'epoch': str(epoch + 1).zfill(3),\n",
        "                                 'mAP@50': str(f'{last[0]:.3f}')})\n",
        "                f.flush()\n",
        "\n",
        "                # Update best mAP\n",
        "                if last[1] > best:\n",
        "                    best = last[1]\n",
        "\n",
        "                # Save model\n",
        "                ckpt = {'model': copy.deepcopy(ema.ema).half()}\n",
        "\n",
        "                # Save last, best and delete\n",
        "                torch.save(ckpt, 'weights/last.pt')\n",
        "                if best == last[1]:\n",
        "                    torch.save(ckpt, 'weights/best.pt')\n",
        "                del ckpt\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        util.strip_optimizer('weights/best.pt')  # strip optimizers\n",
        "        util.strip_optimizer('weights/last.pt')  # strip optimizers\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(args, params, model=None):\n",
        "    import time\n",
        "    from torchvision.ops import nms\n",
        "\n",
        "    def fast_nms(predictions, conf_thres=0.001, iou_thres=0.65):\n",
        "        output = []\n",
        "        for pred in predictions:\n",
        "            pred = pred[pred[:, 4] > conf_thres]\n",
        "            if not pred.shape[0]:\n",
        "                output.append(torch.zeros((0, 6), device=pred.device))\n",
        "                continue\n",
        "            boxes = pred[:, :4]\n",
        "            scores = pred[:, 4]\n",
        "            keep = nms(boxes, scores, iou_thres)\n",
        "            output.append(pred[keep])\n",
        "        return output\n",
        "\n",
        "    # Load validation filenames\n",
        "    filenames = []\n",
        "    with open('/content/val2017.txt') as reader:\n",
        "        for filename in reader.readlines():\n",
        "            filename = filename.rstrip().split('/')[-1]\n",
        "            filenames.append('/content/COCO/images/val2017/' + filename)\n",
        "\n",
        "    # Use small eval set for speed\n",
        "    eval_subset = int(0.1 * len(filenames))  # 10% of val set\n",
        "    filenames = filenames[:eval_subset]\n",
        "\n",
        "    dataset = Dataset(filenames, args.input_size, params, augment=False)\n",
        "    loader = data.DataLoader(dataset, batch_size=8, shuffle=False,\n",
        "                             num_workers=4, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
        "\n",
        "    if model is None:\n",
        "        model = torch.load('weights/best.pt', map_location='cuda')['model'].float()\n",
        "\n",
        "    model.half()\n",
        "    model.eval()\n",
        "\n",
        "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()\n",
        "    n_iou = iou_v.numel()\n",
        "\n",
        "    m_pre = m_rec = map50 = mean_ap = 0.0\n",
        "    metrics = []\n",
        "\n",
        "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
        "    for samples, targets, shapes in p_bar:\n",
        "        samples, targets = samples.cuda(), targets.cuda()\n",
        "        samples = samples.half() / 255\n",
        "\n",
        "        b, _, h, w = samples.shape\n",
        "\n",
        "        # Forward pass with autocast\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(samples)\n",
        "\n",
        "        # Rescale targets\n",
        "        targets[:, 2:] *= torch.tensor([w, h, w, h], device=targets.device)\n",
        "\n",
        "        # Apply NMS\n",
        "        outputs = fast_nms(outputs, conf_thres=0.001, iou_thres=0.65)\n",
        "\n",
        "        for i, output in enumerate(outputs):\n",
        "            labels = targets[targets[:, 0] == i, 1:]\n",
        "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
        "\n",
        "            if output.shape[0] == 0:\n",
        "                if labels.shape[0]:\n",
        "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
        "                continue\n",
        "\n",
        "            detections = output.clone()\n",
        "            util.scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
        "\n",
        "            if labels.shape[0]:\n",
        "                tbox = labels[:, 1:5].clone()\n",
        "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2\n",
        "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2\n",
        "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2\n",
        "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2\n",
        "                util.scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
        "\n",
        "                correct_np = numpy.zeros((detections.shape[0], iou_v.shape[0]), dtype=bool)\n",
        "\n",
        "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
        "                iou = util.box_iou(t_tensor[:, 1:], detections[:, :4])\n",
        "                correct_class = t_tensor[:, 0:1] == detections[:, 5:6]\n",
        "\n",
        "                for j in range(n_iou):\n",
        "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
        "                    if x[0].shape[0]:\n",
        "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]].unsqueeze(1)), dim=1)\n",
        "                        matches = matches.cpu().numpy()\n",
        "                        if matches.shape[0] > 1:\n",
        "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
        "                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
        "                        correct_np[matches[:, 1].astype(int), j] = True\n",
        "\n",
        "                correct = torch.tensor(correct_np, dtype=torch.bool, device=targets.device)\n",
        "\n",
        "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
        "\n",
        "    # Compute final metrics\n",
        "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]\n",
        "    if len(metrics) and metrics[0].any():\n",
        "        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics)\n",
        "\n",
        "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
        "    model.float()  # back to float32 for training\n",
        "\n",
        "    return map50, mean_ap\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input-size', default=640, type=int)\n",
        "    parser.add_argument('--batch-size', default=32, type=int)\n",
        "    parser.add_argument('--local_rank', default=0, type=int)\n",
        "    parser.add_argument('--epochs', default=500, type=int)\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--test', action='store_true')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "    args.local_rank = int(os.getenv('LOCAL_RANK', 0))\n",
        "    args.world_size = int(os.getenv('WORLD_SIZE', 1))\n",
        "\n",
        "    if args.world_size > 1:\n",
        "        torch.cuda.set_device(device=args.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        if not os.path.exists('weights'):\n",
        "            os.makedirs('weights')\n",
        "\n",
        "    util.setup_seed()\n",
        "    util.setup_multi_processes()\n",
        "\n",
        "    with open(os.path.join('utils', 'args.yaml'), errors='ignore') as f:\n",
        "        params = yaml.safe_load(f)\n",
        "\n",
        "    if args.train:\n",
        "        train(args, params)\n",
        "    if args.test:\n",
        "        test(args, params)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "usJtZf-wFh6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45049c1-e694-42ae-e39e-dccd4fec275e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIbBcQhJM0S5",
        "outputId": "9b5cd63b-169b-474a-c96e-c8fab469aabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.5)\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.7.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python3 main.py --train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq5vc1E8Olhw",
        "outputId": "874acd4a-efde-45f2-8d1a-a734416b1423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "     epoch    memory      loss\n",
            "     1/500     8.46G     9.463: 100% 3697/3697 [1:01:06<00:00,  1.01it/s]\n",
            " precision    recall       mAP: 100% 625/625 [01:37<00:00,  6.42it/s]\n",
            "     0.293    0.0537    0.0138\n",
            "\n",
            "     epoch    memory      loss\n",
            "     2/500     8.49G     6.703: 100% 3697/3697 [59:52<00:00,  1.03it/s]\n",
            " precision    recall       mAP: 100% 625/625 [01:30<00:00,  6.93it/s]\n",
            "     0.229      0.14    0.0568\n",
            "\n",
            "     epoch    memory      loss\n",
            "     3/500      8.5G     6.111: 100% 3697/3697 [59:01<00:00,  1.04it/s]\n",
            " precision    recall       mAP:  93% 583/625 [01:26<00:05,  7.02it/s]WARNING ‚ö†Ô∏è NMS time limit 0.900s exceeded\n",
            " precision    recall       mAP: 100% 625/625 [02:24<00:00,  4.33it/s]\n",
            "     0.253     0.193    0.0915\n",
            "\n",
            "     epoch    memory      loss\n",
            "     4/500      8.5G     5.736:  95% 3523/3697 [56:01<02:33,  1.14it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python3 main.py --test\n"
      ],
      "metadata": {
        "id": "Qzm1rxLPRnFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er_JPqEwRnHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --train\n",
        "\n"
      ],
      "metadata": {
        "id": "78lSL72FeDmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3j22FsEOIdHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TsJ2vlhwIdJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1 python3 main.py --test"
      ],
      "metadata": {
        "id": "IHztdoBn5oJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --test"
      ],
      "metadata": {
        "id": "X6j7elt75oL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGAabdTu5oOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}