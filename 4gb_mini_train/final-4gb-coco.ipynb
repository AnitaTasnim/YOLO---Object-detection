{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11682725,"sourceType":"datasetVersion","datasetId":7332359}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install required packages\n!pip install -q scikit-learn\n\n \n","metadata":{"id":"CEzdnJOAZngD","outputId":"2dc70bbc-6e6e-4b11-93d8-b4571ded623b","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:26:41.610603Z","iopub.execute_input":"2025-05-07T11:26:41.610860Z","iopub.status.idle":"2025-05-07T11:26:45.964106Z","shell.execute_reply.started":"2025-05-07T11:26:41.610834Z","shell.execute_reply":"2025-05-07T11:26:45.962848Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/labels/train2017/000000000061.txt') as f:\n    print(f.read())\n","metadata":{"id":"K39ZkFzF24Kc","outputId":"df9d0ebd-83c6-4915-cb91-4613dcfe0eab","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:26:45.966496Z","iopub.execute_input":"2025-05-07T11:26:45.967066Z","iopub.status.idle":"2025-05-07T11:26:45.989151Z","shell.execute_reply.started":"2025-05-07T11:26:45.967037Z","shell.execute_reply":"2025-05-07T11:26:45.988520Z"}},"outputs":[{"name":"stdout","text":"0 0.445688 0.480615 0.075125 0.117295\n0 0.640086 0.471742 0.050828 0.081434\n21 0.643211 0.558852 0.129828 0.097623\n21 0.459703 0.592121 0.221750 0.159242\n0 0.435383 0.458320 0.053453 0.111025\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimg_path = '/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/images/train2017/000000000030.jpg'\nprint(\"Exists?\" , os.path.exists(img_path))\n","metadata":{"id":"MzT3IsFS24M1","outputId":"c72b6dac-31d6-4b1f-d873-43fbb7844b6f","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:26:45.989938Z","iopub.execute_input":"2025-05-07T11:26:45.990202Z","iopub.status.idle":"2025-05-07T11:26:46.010583Z","shell.execute_reply.started":"2025-05-07T11:26:45.990162Z","shell.execute_reply":"2025-05-07T11:26:46.009871Z"}},"outputs":[{"name":"stdout","text":"Exists? True\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ─── 1) Install PyTorch 1.13.1 + CUDA 11.7 and other deps ───────────────────\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 \\\n    --extra-index-url https://download.pytorch.org/whl/cu117\n!pip install opencv-python==4.5.5.64 PyYAML tqdm\n","metadata":{"outputId":"190389ba-ccd5-410e-b1d7-c44f83294b54","id":"cZxCc7y-Wb2N","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:26:46.011362Z","iopub.execute_input":"2025-05-07T11:26:46.011556Z","iopub.status.idle":"2025-05-07T11:27:47.446564Z","shell.execute_reply.started":"2025-05-07T11:26:46.011540Z","shell.execute_reply":"2025-05-07T11:27:47.445541Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu124\nUninstalling torch-2.5.1+cu124:\n  Successfully uninstalled torch-2.5.1+cu124\nFound existing installation: torchvision 0.20.1+cu124\nUninstalling torchvision-0.20.1+cu124:\n  Successfully uninstalled torchvision-0.20.1+cu124\nFound existing installation: torchaudio 2.5.1+cu124\nUninstalling torchaudio-2.5.1+cu124:\n  Successfully uninstalled torchaudio-2.5.1+cu124\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\nCollecting torch==1.13.1+cu117\n  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1801.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m649.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.1+cu117 (from versions: 0.1.6, 0.2.0, 0.15.0+cu117, 0.15.1, 0.15.1+cu117, 0.15.2, 0.15.2+cu117, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.1+cu117\u001b[0m\u001b[31m\n\u001b[0mCollecting opencv-python==4.5.5.64\n  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python==4.5.5.64) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\nDownloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: opencv-python\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.11.0.86\n    Uninstalling opencv-python-4.11.0.86:\n      Successfully uninstalled opencv-python-4.11.0.86\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed opencv-python-4.5.5.64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ─── 1) Install PyTorch 1.13.1 + CUDA 11.7 and other deps ───────────────────\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 \\\n    --extra-index-url https://download.pytorch.org/whl/cu117\n!pip install opencv-python==4.5.5.64 PyYAML tqdm\n","metadata":{"id":"TacLG-33Z3BZ","outputId":"b2135183-4098-4450-f4c0-5399a58a8111","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:27:47.449163Z","iopub.execute_input":"2025-05-07T11:27:47.449492Z","iopub.status.idle":"2025-05-07T11:28:12.210790Z","shell.execute_reply.started":"2025-05-07T11:27:47.449469Z","shell.execute_reply":"2025-05-07T11:28:12.210086Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\nCollecting torch==1.13.1+cu117\n  Using cached https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp311-cp311-linux_x86_64.whl (1801.8 MB)\n\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.15.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.14.1+cu117 (from versions: 0.1.6, 0.2.0, 0.15.0+cu117, 0.15.1, 0.15.1+cu117, 0.15.2, 0.15.2+cu117, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.14.1+cu117\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: opencv-python==4.5.5.64 in /usr/local/lib/python3.11/dist-packages (4.5.5.64)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python==4.5.5.64) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python==4.5.5.64) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python==4.5.5.64) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install torch","metadata":{"id":"QC3v5W8We8Cl","outputId":"95b22bab-dbb6-4543-b095-4438b64b59c1","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:28:12.211734Z","iopub.execute_input":"2025-05-07T11:28:12.212004Z","iopub.status.idle":"2025-05-07T11:30:50.080151Z","shell.execute_reply.started":"2025-05-07T11:28:12.211974Z","shell.execute_reply":"2025-05-07T11:30:50.079072Z"}},"outputs":[{"name":"stdout","text":"Collecting torch\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.0 (from torch)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.7.18 requires torchvision>=0.11, which is not installed.\ntimm 1.0.14 requires torchvision, which is not installed.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install -U numpy","metadata":{"id":"zXrSslEdgxqW","outputId":"498abf1d-a5f4-401a-abd0-cc086ece91ba","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:30:50.081669Z","iopub.execute_input":"2025-05-07T11:30:50.082081Z","iopub.status.idle":"2025-05-07T11:30:56.717944Z","shell.execute_reply.started":"2025-05-07T11:30:50.082036Z","shell.execute_reply":"2025-05-07T11:30:56.716977Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting numpy\n  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.7.18 requires torchvision>=0.11, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.5 which is incompatible.\ncatboost 1.2.7 requires numpy<2.0,>=1.16.0, but you have numpy 2.2.5 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nmatplotlib 3.7.5 requires numpy<2,>=1.20, but you have numpy 2.2.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nthinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.5 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.5 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nlangchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Upgrade to the newest opencv-python wheel (built against NumPy 2.x)\n!pip install --upgrade opencv-python\n","metadata":{"id":"ehQYwA7HhZpY","outputId":"ef6ce6b5-23ce-4eac-8552-ed4f6aaa20bc","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:30:56.719079Z","iopub.execute_input":"2025-05-07T11:30:56.719387Z","iopub.status.idle":"2025-05-07T11:31:02.180352Z","shell.execute_reply.started":"2025-05-07T11:30:56.719361Z","shell.execute_reply":"2025-05-07T11:31:02.179404Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.5.5.64)\nCollecting opencv-python\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.2.5)\nDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: opencv-python\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.5.5.64\n    Uninstalling opencv-python-4.5.5.64:\n      Successfully uninstalled opencv-python-4.5.5.64\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed opencv-python-4.11.0.86\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# utils","metadata":{"id":"HZ8Ny3sxPCfv"}},{"cell_type":"code","source":"import os\n\n# Create the directory if it doesn't exist\nos.makedirs(\"utils\", exist_ok=True)\n","metadata":{"id":"gbtFb1qUqSqJ","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.181522Z","iopub.execute_input":"2025-05-07T11:31:02.181767Z","iopub.status.idle":"2025-05-07T11:31:02.186369Z","shell.execute_reply.started":"2025-05-07T11:31:02.181733Z","shell.execute_reply":"2025-05-07T11:31:02.185660Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## args","metadata":{"id":"940qoVLm42AX"}},{"cell_type":"code","source":"%%writefile utils/args.yaml\nlr0: 0.010                    # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.010                    # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.93700000          # SGD momentum/Adam beta1\nweight_decay: 0.0005          # optimizer weight decay 5e-4\nwarmup_epochs: 3.000          # warmup epochs\nwarmup_momentum: 0.8          # warmup initial momentum\nwarmup_bias_lr: 0.10          # warmup initial bias lr\nbox: 7.5                      # box loss gain\ncls: 0.5                      # cls loss gain\ndfl: 1.5                      # cls loss gain\nhsv_h: 0.015000               # image HSV-Hue augmentation (fraction)\nhsv_s: 0.700000               # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.400000               # image HSV-Value augmentation (fraction)\ndegrees: 0.0000               # image rotation (+/- deg)\ntranslate: 0.10               # image translation (+/- fraction)\nscale: 0.500000               # image scale (+/- gain)\nshear: 0.000000               # image shear (+/- deg)\nflip_ud: 0.0000               # image flip up-down (probability)\nflip_lr: 0.5000               # image flip left-right (probability)\nmosaic: 1.00000               # image mosaic (probability)\nmix_up: 0.00000               # image mix-up (probability)\n# classes\nnames:\n  0: person\n  1: bicycle\n  2: car\n  3: motorcycle\n  4: airplane\n  5: bus\n  6: train\n  7: truck\n  8: boat\n  9: traffic light\n  10: fire hydrant\n  11: stop sign\n  12: parking meter\n  13: bench\n  14: bird\n  15: cat\n  16: dog\n  17: horse\n  18: sheep\n  19: cow\n  20: elephant\n  21: bear\n  22: zebra\n  23: giraffe\n  24: backpack\n  25: umbrella\n  26: handbag\n  27: tie\n  28: suitcase\n  29: frisbee\n  30: skis\n  31: snowboard\n  32: sports ball\n  33: kite\n  34: baseball bat\n  35: baseball glove\n  36: skateboard\n  37: surfboard\n  38: tennis racket\n  39: bottle\n  40: wine glass\n  41: cup\n  42: fork\n  43: knife\n  44: spoon\n  45: bowl\n  46: banana\n  47: apple\n  48: sandwich\n  49: orange\n  50: broccoli\n  51: carrot\n  52: hot dog\n  53: pizza\n  54: donut\n  55: cake\n  56: chair\n  57: couch\n  58: potted plant\n  59: bed\n  60: dining table\n  61: toilet\n  62: tv\n  63: laptop\n  64: mouse\n  65: remote\n  66: keyboard\n  67: cell phone\n  68: microwave\n  69: oven\n  70: toaster\n  71: sink\n  72: refrigerator\n  73: book\n  74: clock\n  75: vase\n  76: scissors\n  77: teddy bear\n  78: hair drier\n  79: toothbrush\n\n","metadata":{"id":"0-2hDyhK3OGa","outputId":"1b0aa2ca-1d86-46b4-d282-106355c090e5","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.187618Z","iopub.execute_input":"2025-05-07T11:31:02.188466Z","iopub.status.idle":"2025-05-07T11:31:02.203560Z","shell.execute_reply.started":"2025-05-07T11:31:02.188432Z","shell.execute_reply":"2025-05-07T11:31:02.202906Z"}},"outputs":[{"name":"stdout","text":"Writing utils/args.yaml\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## dataset","metadata":{"id":"9F85CnjT45Rw"}},{"cell_type":"code","source":"%%writefile utils/dataset.py\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.serialization import add_safe_globals\n\nimport math\nimport os\nimport random\n\nimport cv2\nimport numpy\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\n\nFORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, filenames, input_size, params, augment):\n        self.params = params\n        self.mosaic = augment\n        self.augment = augment\n        self.input_size = input_size\n\n        # Read labels\n        cache = self.load_label(filenames)\n        labels, shapes = zip(*cache.values())\n        self.labels = list(labels)\n        self.shapes = numpy.array(shapes, dtype=numpy.float64)\n        self.filenames = list(cache.keys())  # update\n        self.n = len(shapes)  # number of samples\n        self.indices = range(self.n)\n        # Albumentations (optional, only used if package is installed)\n        self.albumentations = Albumentations()\n\n    def __getitem__(self, index):\n        index = self.indices[index]\n\n        params = self.params\n        mosaic = self.mosaic and random.random() < params['mosaic']\n\n        if mosaic:\n            shapes = None\n            # Load MOSAIC\n            image, label = self.load_mosaic(index, params)\n            # MixUp augmentation\n            if random.random() < params['mix_up']:\n                index = random.choice(self.indices)\n                mix_image1, mix_label1 = image, label\n                mix_image2, mix_label2 = self.load_mosaic(index, params)\n\n                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n        else:\n            # Load image\n            image, shape = self.load_image(index)\n            h, w = image.shape[:2]\n\n            # Resize\n            image, ratio, pad = resize(image, self.input_size, self.augment)\n            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # for COCO mAP rescaling\n\n            label = self.labels[index].copy()\n            if label.size:\n                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n            if self.augment:\n                image, label = random_perspective(image, label, params)\n        nl = len(label)  # number of labels\n        if nl:\n            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n\n        if self.augment:\n            # Albumentations\n            image, label = self.albumentations(image, label)\n            nl = len(label)  # update after albumentations\n            # HSV color-space\n            augment_hsv(image, params)\n            # Flip up-down\n            if random.random() < params['flip_ud']:\n                image = numpy.flipud(image)\n                if nl:\n                    label[:, 2] = 1 - label[:, 2]\n            # Flip left-right\n            if random.random() < params['flip_lr']:\n                image = numpy.fliplr(image)\n                if nl:\n                    label[:, 1] = 1 - label[:, 1]\n\n        target = torch.zeros((nl, 6))\n        if nl:\n            target[:, 1:] = torch.from_numpy(label)\n\n        # Convert HWC to CHW, BGR to RGB\n        sample = image.transpose((2, 0, 1))[::-1]\n        sample = numpy.ascontiguousarray(sample)\n\n        return torch.from_numpy(sample), target, shapes\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def load_image(self, i):\n        image = cv2.imread(self.filenames[i])\n        h, w = image.shape[:2]\n        r = self.input_size / max(h, w)\n        if r != 1:\n            image = cv2.resize(image,\n                               dsize=(int(w * r), int(h * r)),\n                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n        return image, (h, w)\n\n    def load_mosaic(self, index, params):\n        label4 = []\n        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n\n        border = [-self.input_size // 2, -self.input_size // 2]\n\n        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n\n        indices = [index] + random.choices(self.indices, k=3)\n        random.shuffle(indices)\n\n        for i, index in enumerate(indices):\n            # Load image\n            image, _ = self.load_image(index)\n            shape = image.shape\n            if i == 0:  # top left\n                x1a = max(xc - shape[1], 0)\n                y1a = max(yc - shape[0], 0)\n                x2a = xc\n                y2a = yc\n                x1b = shape[1] - (x2a - x1a)\n                y1b = shape[0] - (y2a - y1a)\n                x2b = shape[1]\n                y2b = shape[0]\n            if i == 1:  # top right\n                x1a = xc\n                y1a = max(yc - shape[0], 0)\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = yc\n                x1b = 0\n                y1b = shape[0] - (y2a - y1a)\n                x2b = min(shape[1], x2a - x1a)\n                y2b = shape[0]\n            if i == 2:  # bottom left\n                x1a = max(xc - shape[1], 0)\n                y1a = yc\n                x2a = xc\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = shape[1] - (x2a - x1a)\n                y1b = 0\n                x2b = shape[1]\n                y2b = min(y2a - y1a, shape[0])\n            if i == 3:  # bottom right\n                x1a = xc\n                y1a = yc\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = 0\n                y1b = 0\n                x2b = min(shape[1], x2a - x1a)\n                y2b = min(y2a - y1a, shape[0])\n\n            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            pad_w = x1a - x1b\n            pad_h = y1a - y1b\n\n            # Labels\n            label = self.labels[index].copy()\n            if len(label):\n                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n            label4.append(label)\n\n        # Concat/clip labels\n        label4 = numpy.concatenate(label4, 0)\n        for x in label4[:, 1:]:\n            numpy.clip(x, 0, 2 * self.input_size, out=x)\n\n        # Augment\n        image4, label4 = random_perspective(image4, label4, params, border)\n\n        return image4, label4\n\n    @staticmethod\n    def collate_fn(batch):\n        samples, targets, shapes = zip(*batch)\n        for i, item in enumerate(targets):\n            item[:, 0] = i  # add target image index\n        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n\n    \n    \n    \n    @staticmethod\n\n\n      \n    def load_label(filenames):\n        cache_path = '/kaggle/working/dataset.cache'\n        \n        # Allow numpy unpickling\n        add_safe_globals([np.core.multiarray._reconstruct])\n    \n        if os.path.exists(cache_path):\n            return torch.load(cache_path, weights_only=False)\n    \n        x = {}\n        for filename in filenames:\n            try:\n                # verify image\n                with open(filename, 'rb') as f:\n                    image = Image.open(f)\n                    image.verify()\n                shape = image.size\n                assert shape[0] > 9 and shape[1] > 9, f'image size {shape} <10 pixels'\n                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n    \n                # match image path to label path\n                a = f'{os.sep}images{os.sep}'\n                b = f'{os.sep}labels{os.sep}'\n                label_path = b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'\n    \n                if os.path.isfile(label_path):\n                    with open(label_path) as f:\n                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n                        label = np.array(label, dtype=np.float32)\n                    nl = len(label)\n                    if nl:\n                        assert label.shape[1] == 5, 'labels require 5 columns'\n                        assert (label >= 0).all(), 'negative label values'\n                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n                        _, i = np.unique(label, axis=0, return_index=True)\n                        if len(i) < nl:\n                            label = label[i]\n                    else:\n                        label = np.zeros((0, 5), dtype=np.float32)\n                else:\n                    label = np.zeros((0, 5), dtype=np.float32)\n    \n                x[filename] = [label, shape]\n            except FileNotFoundError:\n                pass\n    \n        torch.save(x, cache_path)\n        return x\n\n    \n\ndef wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n    # Convert nx4 boxes\n    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n    return y\n\n\ndef xy2wh(x, w=640, h=640):\n    # warning: inplace clip\n    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n\n    # Convert nx4 boxes\n    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n    return y\n\n\ndef resample():\n    choices = (cv2.INTER_AREA,\n               cv2.INTER_CUBIC,\n               cv2.INTER_LINEAR,\n               cv2.INTER_NEAREST,\n               cv2.INTER_LANCZOS4)\n    return random.choice(seq=choices)\n\n\ndef augment_hsv(image, params):\n    # HSV color-space augmentation\n    h = params['hsv_h']\n    s = params['hsv_s']\n    v = params['hsv_v']\n\n    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n\n    x = numpy.arange(0, 256, dtype=r.dtype)\n    lut_h = ((x * r[0]) % 180).astype('uint8')\n    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n\n    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n\n\ndef resize(image, input_size, augment):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = image.shape[:2]  # current shape [height, width]\n\n    # Scale ratio (new / old)\n    r = min(input_size / shape[0], input_size / shape[1])\n    if not augment:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    w = (input_size - pad[0]) / 2\n    h = (input_size - pad[1]) / 2\n\n    if shape[::-1] != pad:  # resize\n        image = cv2.resize(image,\n                           dsize=pad,\n                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n    return image, (r, r), (w, h)\n\n\ndef candidates(box1, box2):\n    # box1(4,n), box2(4,n)\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n\n\ndef random_perspective(samples, targets, params, border=(0, 0)):\n    h = samples.shape[0] + border[0] * 2\n    w = samples.shape[1] + border[1] * 2\n\n    # Center\n    center = numpy.eye(3)\n    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n\n    # Perspective\n    perspective = numpy.eye(3)\n\n    # Rotation and Scale\n    rotate = numpy.eye(3)\n    a = random.uniform(-params['degrees'], params['degrees'])\n    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n    # Shear\n    shear = numpy.eye(3)\n    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n\n    # Translation\n    translate = numpy.eye(3)\n    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n\n    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n    matrix = translate @ shear @ rotate @ perspective @ center\n    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n\n    # Transform label coordinates\n    n = len(targets)\n    if n:\n        xy = numpy.ones((n * 4, 3))\n        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n        xy = xy @ matrix.T  # transform\n        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n\n        # create new boxes\n        x = xy[:, [0, 2, 4, 6]]\n        y = xy[:, [1, 3, 5, 7]]\n        new = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n        # clip\n        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n\n        # filter candidates\n        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n        targets = targets[indices]\n        targets[:, 1:5] = new[indices]\n\n    return samples, targets\n\n\ndef mix_up(image1, label1, image2, label2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    alpha = numpy.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n    label = numpy.concatenate((label1, label2), 0)\n    return image, label\n\n\nclass Albumentations:\n    def __init__(self):\n        self.transform = None\n        try:\n            import albumentations as album\n\n            transforms = [album.Blur(p=0.01),\n                          album.CLAHE(p=0.01),\n                          album.ToGray(p=0.01),\n                          album.MedianBlur(p=0.01)]\n            self.transform = album.Compose(transforms,\n                                           album.BboxParams('yolo', ['class_labels']))\n\n        except ImportError:  # package not installed, skip\n            pass\n\n    def __call__(self, image, label):\n        if self.transform:\n            x = self.transform(image=image,\n                               bboxes=label[:, 1:],\n                               class_labels=label[:, 0])\n            image = x['image']\n            label = numpy.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n        return image, label","metadata":{"id":"O7Jo-w4HIuHf","outputId":"606d346b-4995-4749-f3a5-450f3057a151","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.204478Z","iopub.execute_input":"2025-05-07T11:31:02.205096Z","iopub.status.idle":"2025-05-07T11:31:02.221785Z","shell.execute_reply.started":"2025-05-07T11:31:02.205077Z","shell.execute_reply":"2025-05-07T11:31:02.221120Z"}},"outputs":[{"name":"stdout","text":"Writing utils/dataset.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# New Section","metadata":{"id":"iIFZ-iadQYt4"}},{"cell_type":"markdown","source":"## util","metadata":{"id":"tWbLRgbarSLx"}},{"cell_type":"code","source":"%%writefile utils/util.py\n\n\n\nimport copy\nimport math\nimport random\nimport time\n\nimport numpy\nimport torch\nimport torchvision\nfrom torch.nn.functional import cross_entropy, one_hot\n\n\ndef setup_seed():\n    \"\"\"\n    Setup random seed.\n    \"\"\"\n    random.seed(0)\n    numpy.random.seed(0)\n    torch.manual_seed(0)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef setup_multi_processes():\n    \"\"\"\n    Setup multi-processing environment variables.\n    \"\"\"\n    import cv2\n    from os import environ\n    from platform import system\n\n    # set multiprocess start method as `fork` to speed up the training\n    if system() != 'Windows':\n        torch.multiprocessing.set_start_method('fork', force=True)\n\n    # disable opencv multithreading to avoid system being overloaded\n    cv2.setNumThreads(0)\n\n    # setup OMP threads\n    if 'OMP_NUM_THREADS' not in environ:\n        environ['OMP_NUM_THREADS'] = '1'\n\n    # setup MKL threads\n    if 'MKL_NUM_THREADS' not in environ:\n        environ['MKL_NUM_THREADS'] = '1'\n\n\ndef scale(coords, shape1, shape2, ratio_pad=None):\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n\n    coords[:, 0].clamp_(0, shape2[1])  # x1\n    coords[:, 1].clamp_(0, shape2[0])  # y1\n    coords[:, 2].clamp_(0, shape2[1])  # x2\n    coords[:, 3].clamp_(0, shape2[0])  # y2\n    return coords\n\n\ndef make_anchors(x, strides, offset=0.5):\n    \"\"\"\n    Generate anchors from features\n    \"\"\"\n    assert x is not None\n    anchor_points, stride_tensor = [], []\n    for i, stride in enumerate(strides):\n        _, _, h, w = x[i].shape\n        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n        sy, sx = torch.meshgrid(sy, sx)\n        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n    return torch.cat(anchor_points), torch.cat(stride_tensor)\n\n\ndef box_iou(box1, box2):\n    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    \"\"\"\n    Return intersection-over-union (Jaccard index) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Arguments:\n        box1 (Tensor[N, 4])\n        box2 (Tensor[M, 4])\n    Returns:\n        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n            IoU values for every element in boxes1 and boxes2\n    \"\"\"\n\n    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n\n    # IoU = intersection / (area1 + area2 - intersection)\n    box1 = box1.T\n    box2 = box2.T\n\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    return intersection / (area1[:, None] + area2 - intersection)\n\n\ndef wh2xy(x):\n    y = x.clone()\n    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n    return y\n\n\ndef non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n    nc = prediction.shape[1] - 4  # number of classes\n    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n\n    # Settings\n    max_wh = 7680  # (pixels) maximum box width and height\n    max_det = 300  # the maximum number of boxes to keep after NMS\n    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n\n    start = time.time()\n    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for index, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        x = x.transpose(0, -1)[xc[index]]  # confidence\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Detections matrix nx6 (box, conf, cls)\n        box, cls = x.split((4, nc), 1)\n        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n        box = wh2xy(box)\n        if nc > 1:\n            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = cls.max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n        # Check shape\n        if not x.shape[0]:  # no boxes\n            continue\n        # sort by confidence and remove excess boxes\n        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n\n        # Batched NMS\n        c = x[:, 5:6] * max_wh  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n        i = i[:max_det]  # limit detections\n        outputs[index] = x[i]\n        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n            break  # time limit exceeded\n\n    return outputs\n\n\ndef smooth(y, f=0.05):\n    # Box filter of fraction f\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = numpy.ones(nf // 2)  # ones padding\n    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n\n\ndef compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n    \"\"\"\n    Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:  True positives (nparray, nx1 or nx10).\n        conf:  Object-ness value from 0-1 (nparray).\n        pred_cls:  Predicted object classes (nparray).\n        target_cls:  True object classes (nparray).\n    # Returns\n        The average precision\n    \"\"\"\n    # Sort by object-ness\n    i = numpy.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    p = numpy.zeros((nc, 1000))\n    r = numpy.zeros((nc, 1000))\n    ap = numpy.zeros((nc, tp.shape[1]))\n    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        nl = nt[ci]  # number of labels\n        no = i.sum()  # number of outputs\n        if no == 0 or nl == 0:\n            continue\n\n        # Accumulate FPs and TPs\n        fpc = (1 - tp[i]).cumsum(0)\n        tpc = tp[i].cumsum(0)\n\n        # Recall\n        recall = tpc / (nl + eps)  # recall curve\n        # negative x, xp because xp decreases\n        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n\n        # Precision\n        precision = tpc / (tpc + fpc)  # precision curve\n        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n\n        # AP from recall-precision curve\n        for j in range(tp.shape[1]):\n            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n\n            # Compute the precision envelope\n            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n\n            # Integrate area under curve\n            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1 = 2 * p * r / (p + r + eps)\n\n    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n    tp = (r * nt).round()  # true positives\n    fp = (tp / (p + eps) - tp).round()  # false positives\n    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n    m_pre, m_rec = p.mean(), r.mean()\n    map50, mean_ap = ap50.mean(), ap.mean()\n    return tp, fp, m_pre, m_rec, map50, mean_ap\n\n\ndef strip_optimizer(filename):\n    x = torch.load(filename, map_location=torch.device('cuda'), weights_only=False)\n\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():\n        p.requires_grad = False\n    torch.save(x, filename)\n\n\ndef clip_gradients(model, max_norm=10.0):\n    parameters = model.parameters()\n    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n\n\nclass EMA:\n    \"\"\"\n    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    \"\"\"\n\n    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n        # Create EMA\n        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n        self.updates = updates  # number of EMA updates\n        # decay exponential ramp (to help early epochs)\n        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        if hasattr(model, 'module'):\n            model = model.module\n        # Update EMA parameters\n        with torch.no_grad():\n            self.updates += 1\n            d = self.decay(self.updates)\n\n            msd = model.state_dict()  # model state_dict\n            for k, v in self.ema.state_dict().items():\n                if v.dtype.is_floating_point:\n                    v *= d\n                    v += (1 - d) * msd[k].detach()\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.num = 0\n        self.sum = 0\n        self.avg = 0\n\n    def update(self, v, n):\n        if not math.isnan(float(v)):\n            self.num = self.num + n\n            self.sum = self.sum + v * n\n            self.avg = self.sum / self.num\n\n\nclass ComputeLoss:\n    def __init__(self, model, params):\n        super().__init__()\n        if hasattr(model, 'module'):\n            model = model.module\n\n        device = next(model.parameters()).device  # get model device\n\n        m = model.head  # Head() module\n        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n        self.stride = m.stride  # model strides\n        self.nc = m.nc  # number of classes\n        self.no = m.no\n        self.device = device\n        self.params = params\n\n        # task aligned assigner\n        self.top_k = 10\n        self.alpha = 0.5\n        self.beta = 6.0\n        self.eps = 1e-9\n\n        self.bs = 1\n        self.num_max_boxes = 0\n        # DFL Loss params\n        self.dfl_ch = m.dfl.ch\n        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n\n    def __call__(self, outputs, targets):\n        x = outputs[1] if isinstance(outputs, tuple) else outputs\n        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n\n        pred_output = pred_output.permute(0, 2, 1).contiguous()\n        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n\n        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n        size = size * self.stride[0]\n\n        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n\n        # targets\n        if targets.shape[0] == 0:\n            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n        else:\n            i = targets[:, 0]  # image index\n            _, counts = i.unique(return_counts=True)\n            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n            for j in range(pred_scores.shape[0]):\n                matches = i == j\n                n = matches.sum()\n                if n:\n                    gt[j, :n] = targets[matches, 1:]\n            gt[..., 1:5] = wh2xy(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n\n        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n\n        # boxes\n        b, a, c = pred_output.shape\n        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n\n        a, b = torch.split(pred_bboxes, 2, -1)\n        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n\n        scores = pred_scores.detach().sigmoid()\n        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n                                                            gt_labels, gt_bboxes, mask_gt,\n                                                            anchor_points * stride_tensor)\n\n        target_bboxes /= stride_tensor\n        target_scores_sum = target_scores.sum()\n\n        # cls loss\n        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n        loss_cls = loss_cls.sum() / target_scores_sum\n\n        # box loss\n        loss_box = torch.zeros(1, device=self.device)\n        loss_dfl = torch.zeros(1, device=self.device)\n        if fg_mask.sum():\n            # IoU loss\n            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n            # DFL loss\n            a, b = torch.split(target_bboxes, 2, -1)\n            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n\n        loss_cls *= self.params['cls']\n        loss_box *= self.params['box']\n        loss_dfl *= self.params['dfl']\n        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n\n    @torch.no_grad()\n    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n        \"\"\"\n        Task-aligned One-stage Object Detection assigner\n        \"\"\"\n        self.bs = pred_scores.size(0)\n        self.num_max_boxes = true_bboxes.size(1)\n\n        if self.num_max_boxes == 0:\n            device = true_bboxes.device\n            return (torch.full_like(pred_scores[..., 0], self.nc).to(device),\n                    torch.zeros_like(pred_bboxes).to(device),\n                    torch.zeros_like(pred_scores).to(device),\n                    torch.zeros_like(pred_scores[..., 0]).to(device),\n                    torch.zeros_like(pred_scores[..., 0]).to(device))\n\n        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n        i[1] = true_labels.long().squeeze(-1)\n\n        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n        overlaps = overlaps.squeeze(3).clamp(0)\n        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n        bs, n_boxes, _ = true_bboxes.shape\n        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n        metrics = align_metric * mask_in_gts\n        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n        num_anchors = metrics.shape[-1]\n        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n        if top_k_mask is None:\n            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n        # filter invalid boxes\n        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n        mask_top_k = is_in_top_k.to(metrics.dtype)\n        # merge all mask to a final mask, (b, max_num_obj, h*w)\n        mask_pos = mask_top_k * mask_in_gts * true_mask\n\n        fg_mask = mask_pos.sum(-2)\n        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n            max_overlaps_idx = overlaps.argmax(1)\n            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n            fg_mask = mask_pos.sum(-2)\n        # find each grid serve which gt(index)\n        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n\n        # assigned target labels, (b, 1)\n        batch_index = torch.arange(end=self.bs,\n                                   dtype=torch.int64,\n                                   device=true_labels.device)[..., None]\n        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n        target_labels = true_labels.long().flatten()[target_gt_idx]\n\n        # assigned target boxes\n        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n\n        # assigned target scores\n        target_labels.clamp(0)\n        target_scores = one_hot(target_labels, self.nc)\n        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n\n        # normalize\n        align_metric *= mask_pos\n        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n        norm_align_metric = norm_align_metric.unsqueeze(-1)\n        target_scores = target_scores * norm_align_metric\n\n        return target_bboxes, target_scores, fg_mask.bool()\n\n    @staticmethod\n    def df_loss(pred_dist, target):\n        # Return sum of left and right DFL losses\n        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n        tl = target.long()  # target left\n        tr = tl + 1  # target right\n        wl = tr - target  # weight left\n        wr = 1 - wl  # weight right\n        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n\n    @staticmethod\n    def iou(box1, box2, eps=1e-7):\n        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n\n        # Intersection area\n        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n        intersection = area1.clamp(0) * area2.clamp(0)\n\n        # Union Area\n        union = w1 * h1 + w2 * h2 - intersection + eps\n\n        # IoU\n        iou = intersection / union\n        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n        # Complete IoU https://arxiv.org/abs/1911.08287v1\n        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n        # center dist ** 2\n        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n        with torch.no_grad():\n            alpha = v / (v - iou + (1 + eps))\n        return iou - (rho2 / c2 + v * alpha)  # CIoU","metadata":{"id":"izCZhvAKC92F","outputId":"34801ade-0954-4cc6-ab6f-37b003caa95b","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.222722Z","iopub.execute_input":"2025-05-07T11:31:02.222956Z","iopub.status.idle":"2025-05-07T11:31:02.245295Z","shell.execute_reply.started":"2025-05-07T11:31:02.222941Z","shell.execute_reply":"2025-05-07T11:31:02.244668Z"}},"outputs":[{"name":"stdout","text":"Writing utils/util.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# nets","metadata":{"id":"en5AbGCXFpcj"}},{"cell_type":"code","source":"import os\n\n# Create the directory if it doesn't exist\nos.makedirs(\"nets\", exist_ok=True)\n \n\n\n","metadata":{"id":"YknSrS8JwUCW","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.246140Z","iopub.execute_input":"2025-05-07T11:31:02.246423Z","iopub.status.idle":"2025-05-07T11:31:02.259380Z","shell.execute_reply.started":"2025-05-07T11:31:02.246399Z","shell.execute_reply":"2025-05-07T11:31:02.258786Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"%%writefile nets/nn.py\n\n\n\n\n\n\nimport math\n\nimport torch\n\nfrom utils.util import make_anchors\n\n\ndef pad(k, p=None, d=1):\n    if d > 1:\n        k = d * (k - 1) + 1\n    if p is None:\n        p = k // 2\n    return p\n\n\ndef fuse_conv(conv, norm):\n    fused_conv = torch.nn.Conv2d(conv.in_channels,\n                                 conv.out_channels,\n                                 kernel_size=conv.kernel_size,\n                                 stride=conv.stride,\n                                 padding=conv.padding,\n                                 groups=conv.groups,\n                                 bias=True).requires_grad_(False).to(conv.weight.device)\n\n    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n\n    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n\n    return fused_conv\n\n\nclass Conv(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n        self.relu = torch.nn.SiLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(self.norm(self.conv(x)))\n\n    def fuse_forward(self, x):\n        return self.relu(self.conv(x))\n\n\nclass Residual(torch.nn.Module):\n    def __init__(self, ch, add=True):\n        super().__init__()\n        self.add_m = add\n        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n                                         Conv(ch, ch, 3))\n\n    def forward(self, x):\n        return self.res_m(x) + x if self.add_m else self.res_m(x)\n\n\nclass CSP(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, n=1, add=True):\n        super().__init__()\n        self.conv1 = Conv(in_ch, out_ch // 2)\n        self.conv2 = Conv(in_ch, out_ch // 2)\n        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n\n    def forward(self, x):\n        y = [self.conv1(x), self.conv2(x)]\n        y.extend(m(y[-1]) for m in self.res_m)\n        return self.conv3(torch.cat(y, dim=1))\n\n\nclass SPP(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, k=5):\n        super().__init__()\n        self.conv1 = Conv(in_ch, in_ch // 2)\n        self.conv2 = Conv(in_ch * 2, out_ch)\n        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        y1 = self.res_m(x)\n        y2 = self.res_m(y1)\n        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n\n\nclass DarkNet(torch.nn.Module):\n    def __init__(self, width, depth):\n        super().__init__()\n        p1 = [Conv(width[0], width[1], 3, 2)]\n        p2 = [Conv(width[1], width[2], 3, 2),\n              CSP(width[2], width[2], depth[0])]\n        p3 = [Conv(width[2], width[3], 3, 2),\n              CSP(width[3], width[3], depth[1])]\n        p4 = [Conv(width[3], width[4], 3, 2),\n              CSP(width[4], width[4], depth[2])]\n        p5 = [Conv(width[4], width[5], 3, 2),\n              CSP(width[5], width[5], depth[0]),\n              SPP(width[5], width[5])]\n\n        self.p1 = torch.nn.Sequential(*p1)\n        self.p2 = torch.nn.Sequential(*p2)\n        self.p3 = torch.nn.Sequential(*p3)\n        self.p4 = torch.nn.Sequential(*p4)\n        self.p5 = torch.nn.Sequential(*p5)\n\n    def forward(self, x):\n        p1 = self.p1(x)\n        p2 = self.p2(p1)\n        p3 = self.p3(p2)\n        p4 = self.p4(p3)\n        p5 = self.p5(p4)\n        return p3, p4, p5\n\n\nclass DarkFPN(torch.nn.Module):\n    def __init__(self, width, depth):\n        super().__init__()\n        self.up = torch.nn.Upsample(None, 2)\n        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n        self.h3 = Conv(width[3], width[3], 3, 2)\n        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n        self.h5 = Conv(width[4], width[4], 3, 2)\n        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n\n    def forward(self, x):\n        p3, p4, p5 = x\n        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n        return h2, h4, h6\n\n\nclass DFL(torch.nn.Module):\n    # Integral module of Distribution Focal Loss (DFL)\n    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n    def __init__(self, ch=16):\n        super().__init__()\n        self.ch = ch\n        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n        self.conv.weight.data[:] = torch.nn.Parameter(x)\n\n    def forward(self, x):\n        b, c, a = x.shape\n        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n        return self.conv(x.softmax(1)).view(b, 4, a)\n\n\nclass Head(torch.nn.Module):\n    anchors = torch.empty(0)\n    strides = torch.empty(0)\n\n    def __init__(self, nc=80, filters=()):\n        super().__init__()\n        self.ch = 16  # DFL channels\n        self.nc = nc  # number of classes\n        self.nl = len(filters)  # number of detection layers\n        self.no = nc + self.ch * 4  # number of outputs per anchor\n        self.stride = torch.zeros(self.nl)  # strides computed during build\n\n        c1 = max(filters[0], self.nc)\n        c2 = max((filters[0] // 4, self.ch * 4))\n\n        self.dfl = DFL(self.ch)\n        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n                                                           Conv(c1, c1, 3),\n                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n                                                           Conv(c2, c2, 3),\n                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n\n    def forward(self, x):\n        for i in range(self.nl):\n            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n        if self.training:\n            return x\n        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n\n        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n        box, cls = x.split((self.ch * 4, self.nc), 1)\n        a, b = torch.split(self.dfl(box), 2, 1)\n        a = self.anchors.unsqueeze(0) - a\n        b = self.anchors.unsqueeze(0) + b\n        box = torch.cat(((a + b) / 2, b - a), 1)\n        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n\n    def initialize_biases(self):\n        # Initialize biases\n        # WARNING: requires stride availability\n        m = self\n        for a, b, s in zip(m.box, m.cls, m.stride):\n            a[-1].bias.data[:] = 1.0  # box\n            # cls (.01 objects, 80 classes, 640 img)\n            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n\n\nclass YOLO(torch.nn.Module):\n    def __init__(self, width, depth, num_classes):\n        super().__init__()\n        self.net = DarkNet(width, depth)\n        self.fpn = DarkFPN(width, depth)\n\n        img_dummy = torch.zeros(1, 3, 256, 256)\n        self.head = Head(num_classes, (width[3], width[4], width[5]))\n        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n        self.stride = self.head.stride\n        self.head.initialize_biases()\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.fpn(x)\n        return self.head(list(x))\n\n    def fuse(self):\n        for m in self.modules():\n            if type(m) is Conv and hasattr(m, 'norm'):\n                m.conv = fuse_conv(m.conv, m.norm)\n                m.forward = m.fuse_forward\n                delattr(m, 'norm')\n        return self\n\n\ndef yolo_v8_n(num_classes: int = 80):\n    depth = [1, 2, 2]\n    width = [3, 16, 32, 64, 128, 256]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_s(num_classes: int = 80):\n    depth = [1, 2, 2]\n    width = [3, 32, 64, 128, 256, 512]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_m(num_classes: int = 80):\n    depth = [2, 4, 4]\n    width = [3, 48, 96, 192, 384, 576]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_l(num_classes: int = 80):\n    depth = [3, 6, 6]\n    width = [3, 64, 128, 256, 512, 512]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_x(num_classes: int = 80):\n    depth = [3, 6, 6]\n    width = [3, 80, 160, 320, 640, 640]\n    return YOLO(width, depth, num_classes)","metadata":{"id":"Ppv6-iXzFh1D","outputId":"3ea571c4-7076-43eb-fe02-82d6de08951c","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.262903Z","iopub.execute_input":"2025-05-07T11:31:02.263135Z","iopub.status.idle":"2025-05-07T11:31:02.270643Z","shell.execute_reply.started":"2025-05-07T11:31:02.263119Z","shell.execute_reply":"2025-05-07T11:31:02.270071Z"}},"outputs":[{"name":"stdout","text":"Writing nets/nn.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"id":"Jw3GNNH5Fh3s","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.271458Z","iopub.execute_input":"2025-05-07T11:31:02.271712Z","iopub.status.idle":"2025-05-07T11:31:02.301069Z","shell.execute_reply.started":"2025-05-07T11:31:02.271689Z","shell.execute_reply":"2025-05-07T11:31:02.300546Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# main","metadata":{"id":"u6vWT3kuIunE"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchvision\n","metadata":{"id":"zIbBcQhJM0S5","outputId":"9b5cd63b-169b-474a-c96e-c8fab469aabb","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:02.301940Z","iopub.execute_input":"2025-05-07T11:31:02.302179Z","iopub.status.idle":"2025-05-07T11:31:06.281414Z","shell.execute_reply.started":"2025-05-07T11:31:02.302158Z","shell.execute_reply":"2025-05-07T11:31:06.280721Z"}},"outputs":[{"name":"stdout","text":"Collecting torchvision\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.5)\nRequirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.7.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (4.13.1)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (1.11.1.6)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->torchvision) (3.3.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->torchvision) (75.1.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\nDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchvision\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torchvision-0.22.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%writefile main.py\n\nimport argparse\nimport copy\nimport csv\nimport os\nimport warnings\n\nimport numpy\nimport torch\nimport tqdm\nimport yaml\nfrom torch.utils import data\n\nfrom nets import nn\nfrom utils import util\nfrom utils.dataset import Dataset\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef learning_rate(args, params):\n    def fn(x):\n        return (1 - x / args.epochs) * (1.0 - params['lrf']) + params['lrf']\n\n    return fn\n\n\ndef train(args, params):\n    # Model\n    model = nn.yolo_v8_n(len(params['names'].values())).cuda()\n\n    # Optimizer\n    accumulate = max(round(64 / (args.batch_size * args.world_size)), 1)\n    params['weight_decay'] *= args.batch_size * args.world_size * accumulate / 64\n\n    p = [], [], []\n    for v in model.modules():\n        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n            p[2].append(v.bias)\n        if isinstance(v, torch.nn.BatchNorm2d):\n            p[1].append(v.weight)\n        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n            p[0].append(v.weight)\n\n    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n\n    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n    optimizer.add_param_group({'params': p[1]})\n    del p\n\n    # Scheduler\n    lr = learning_rate(args, params)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr, last_epoch=-1)\n\n    # EMA\n    \n    #if (epoch + 1) % 1 == 0:\n    if args.local_rank == 0:\n        ema = util.EMA(model)\n    else:\n        None\n\n\n  \n    # For training images\n    filenames = []\n    with open('/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/train2017.txt') as reader:\n        for line in reader:\n            # Extract filename only\n            fname = line.strip().split('/')[-1]\n            # Reconstruct the correct path\n            full_path = f'/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/images/train2017/{fname}'\n            filenames.append(full_path)\n\n    \n    dataset = Dataset(filenames, args.input_size, params, True)\n\n \n\n    if args.world_size <= 1:\n        sampler = None\n    else:\n        sampler = data.distributed.DistributedSampler(dataset)\n\n    loader = data.DataLoader(dataset, args.batch_size, sampler is None, sampler,\n                             num_workers=8, pin_memory=True, collate_fn=Dataset.collate_fn)\n\n    if args.world_size > 1:\n        # DDP mode\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        model = torch.nn.parallel.DistributedDataParallel(module=model,\n                                                          device_ids=[args.local_rank],\n                                                          output_device=args.local_rank)\n\n    # Start training\n    best = 0\n    num_batch = len(loader)\n    amp_scale = torch.cuda.amp.GradScaler()\n    criterion = util.ComputeLoss(model, params)\n    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n\n\n    os.makedirs(\"weights\", exist_ok=True)\n \n\n\n    \n    with open('weights/step.csv', 'w') as f:\n        if args.local_rank == 0:\n            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n            writer.writeheader()\n        for epoch in range(args.epochs):\n            model.train()\n\n            if args.epochs - epoch == 10:\n                loader.dataset.mosaic = False\n\n            m_loss = util.AverageMeter()\n            if args.world_size > 1:\n                sampler.set_epoch(epoch)\n            p_bar = enumerate(loader)\n            if (epoch + 1) % 1 == 0:\n                if args.local_rank == 0:\n                    print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n            if (epoch + 1) % 1 == 0:\n                if args.local_rank == 0:\n                    p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n\n            optimizer.zero_grad()\n\n            for i, (samples, targets, _) in p_bar:\n                x = i + num_batch * epoch  # number of iterations\n                samples = samples.cuda().float() / 255\n                targets = targets.cuda()\n\n                # Warmup\n                if x <= num_warmup:\n                    xp = [0, num_warmup]\n                    fp = [1, 64 / (args.batch_size * args.world_size)]\n                    accumulate = max(1, numpy.interp(x, xp, fp).round())\n                    for j, y in enumerate(optimizer.param_groups):\n                        if j == 0:\n                            fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n                        else:\n                            fp = [0.0, y['initial_lr'] * lr(epoch)]\n                        y['lr'] = numpy.interp(x, xp, fp)\n                        if 'momentum' in y:\n                            fp = [params['warmup_momentum'], params['momentum']]\n                            y['momentum'] = numpy.interp(x, xp, fp)\n\n                # Forward\n                with torch.cuda.amp.autocast():\n                    outputs = model(samples)  # forward\n                loss = criterion(outputs, targets)\n\n                m_loss.update(loss.item(), samples.size(0))\n\n                loss *= args.batch_size  # loss scaled by batch_size\n                loss *= args.world_size  # gradient averaged between devices in DDP mode\n\n                # Backward\n                amp_scale.scale(loss).backward()\n\n                # Optimize\n                if x % accumulate == 0:\n                    amp_scale.unscale_(optimizer)  # unscale gradients\n                    util.clip_gradients(model)  # clip gradients\n                    amp_scale.step(optimizer)  # optimizer.step\n                    amp_scale.update()\n                    optimizer.zero_grad()\n                    if ema:\n                        ema.update(model)\n                    torch.cuda.empty_cache()\n\n                # Log\n                if (epoch + 1) % 1 == 0:\n                    if args.local_rank == 0:\n                        memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n                        s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args.epochs}', memory, m_loss.avg)\n                        p_bar.set_description(s)\n\n                del loss\n                del outputs\n                #torch.cuda.empty_cache()\n\n            # Scheduler\n            scheduler.step()\n            if (epoch + 1) % 1 == 0:\n                if args.local_rank == 0:\n                    # mAP\n                    last = test(args, params, ema.ema)\n                    writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n                                     'epoch': str(epoch + 1).zfill(3),\n                                     'mAP@50': str(f'{last[0]:.3f}')})\n                    f.flush()\n    \n                    # Update best mAP\n                    if last[1] > best:\n                        best = last[1]\n    \n                    # Save model\n                    ckpt = {'model': copy.deepcopy(ema.ema).half()}\n    \n                    # Save last, best and delete\n                    torch.save(ckpt, './weights/last.pt')\n                    if best == last[1]:\n                        torch.save(ckpt, './weights/best.pt')\n                    del ckpt\n                    torch.cuda.empty_cache()\n\n    if args.local_rank == 0:\n        util.strip_optimizer('./weights/best.pt')  # strip optimizers\n        util.strip_optimizer('./weights/last.pt')  # strip optimizers\n\n    torch.cuda.empty_cache()\n\n\n@torch.no_grad()\ndef test(args, params, model=None):\n\n\n    filenames = []\n    # For validation images\n \n    with open('/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/val2017.txt') as reader:\n        for line in reader:\n            fname = line.strip().split('/')[-1]\n            full_path = f'/kaggle/input/coc-yolo/COCO_yolo_dataset_cleaned/images/val2017/{fname}'\n            filenames.append(full_path)\n\n \n    \n    dataset = Dataset(filenames, args.input_size, params, False)\n\n    \n \n\n                      \n    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n                             pin_memory=True, collate_fn=Dataset.collate_fn)\n\n    if model is None:\n        model = torch.load('./weights/best.pt', map_location='cuda', weights_only=False)['model'].float()\n\n\n    model.half()\n    model.eval()\n\n    # Configure\n    iou_v = torch.linspace(0.5, 0.95, 10).cuda()  # iou vector for mAP@0.5:0.95\n    n_iou = iou_v.numel()\n\n    m_pre = 0.\n    m_rec = 0.\n    map50 = 0.\n    mean_ap = 0.\n    metrics = []\n    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n    for samples, targets, shapes in p_bar:\n        samples = samples.cuda()\n        targets = targets.cuda()\n        samples = samples.half()  # uint8 to fp16/32\n        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n        _, _, height, width = samples.shape  # batch size, channels, height, width\n\n        # Inference\n        outputs = model(samples)\n\n        # NMS\n        targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n        outputs = util.non_max_suppression(outputs, 0.001, 0.65)\n\n        # Metrics\n        for i, output in enumerate(outputs):\n            labels = targets[targets[:, 0] == i, 1:]\n            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n\n            if output.shape[0] == 0:\n                if labels.shape[0]:\n                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n                continue\n\n            detections = output.clone()\n            util.scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n\n            # Evaluate\n            if labels.shape[0]:\n                tbox = labels[:, 1:5].clone()  # target boxes\n                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n                util.scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n\n                correct = numpy.zeros((detections.shape[0], iou_v.shape[0]))\n                correct = correct.astype(bool)\n\n                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n                iou = util.box_iou(t_tensor[:, 1:], detections[:, :4])\n                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n                for j in range(len(iou_v)):\n                    x = torch.where((iou >= iou_v[j]) & correct_class)\n                    if x[0].shape[0]:\n                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n                        matches = matches.cpu().numpy()\n                        if x[0].shape[0] > 1:\n                            matches = matches[matches[:, 2].argsort()[::-1]]\n                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n                        correct[matches[:, 1].astype(int), j] = True\n                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n\n    # Compute metrics\n    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n    if len(metrics) and metrics[0].any():\n        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics)\n\n    # Print results\n    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n\n    # Return results\n    model.float()  # for training\n    torch.cuda.empty_cache()\n    return map50, mean_ap\n\n\n\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input-size', default=640, type=int)\n    parser.add_argument('--batch-size', default=32, type=int)\n    parser.add_argument('--local_rank', default=0, type=int)\n    parser.add_argument('--epochs', default=100, type=int)\n    parser.add_argument('--train', action='store_true')\n    parser.add_argument('--test', action='store_true')\n\n    args = parser.parse_args()\n\n    args.local_rank = int(os.getenv('LOCAL_RANK', 0))\n    args.world_size = int(os.getenv('WORLD_SIZE', 1))\n\n    if args.world_size > 1:\n        torch.cuda.set_device(device=args.local_rank)\n        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n\n    if args.local_rank == 0:\n        if not os.path.exists('weights'):\n            os.makedirs('weights')\n\n    util.setup_seed()\n    util.setup_multi_processes()\n\n    with open(os.path.join('utils', 'args.yaml'), errors='ignore') as f:\n        params = yaml.safe_load(f)\n\n    if args.train:\n        train(args, params)\n    if args.test:\n        test(args, params)\n        torch.cuda.empty_cache()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"id":"usJtZf-wFh6b","outputId":"c45049c1-e694-42ae-e39e-dccd4fec275e","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:06.282521Z","iopub.execute_input":"2025-05-07T11:31:06.282765Z","iopub.status.idle":"2025-05-07T11:31:06.294028Z","shell.execute_reply.started":"2025-05-07T11:31:06.282742Z","shell.execute_reply":"2025-05-07T11:31:06.293419Z"}},"outputs":[{"name":"stdout","text":"Writing main.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print('abc')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:06.294786Z","iopub.execute_input":"2025-05-07T11:31:06.295049Z","iopub.status.idle":"2025-05-07T11:31:06.310782Z","shell.execute_reply.started":"2025-05-07T11:31:06.295026Z","shell.execute_reply":"2025-05-07T11:31:06.310175Z"}},"outputs":[{"name":"stdout","text":"abc\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install numpy==1.23.5 --force-reinstall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:06.311577Z","iopub.execute_input":"2025-05-07T11:31:06.312255Z","iopub.status.idle":"2025-05-07T11:31:12.392715Z","shell.execute_reply.started":"2025-05-07T11:31:06.312232Z","shell.execute_reply":"2025-05-07T11:31:12.391669Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.2.5\n    Uninstalling numpy-2.2.5:\n      Successfully uninstalled numpy-2.2.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nalbumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nchex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\nibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\npymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nlangchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.23.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntreescope 0.1.8 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\njaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\njax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nxarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.1.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!python3 main.py --train\n","metadata":{"id":"Tq5vc1E8Olhw","outputId":"874acd4a-efde-45f2-8d1a-a734416b1423","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T11:31:12.393817Z","iopub.execute_input":"2025-05-07T11:31:12.394046Z","execution_failed":"2025-05-07T21:48:45.423Z"}},"outputs":[{"name":"stdout","text":"\n     epoch    memory      loss\n     1/100    0.715G     11.66: 100%|█████████| 608/608 [05:18<00:00,  1.91it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:38<00:00, 15.33it/s]\n   0.00176    0.0279  0.000363\n\n     epoch    memory      loss\n     2/100     1.99G     8.631: 100%|█████████| 608/608 [05:08<00:00,  1.97it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:56<00:00, 13.80it/s]\n     0.271    0.0283   0.00635\n\n     epoch    memory      loss\n     3/100     1.98G     7.472: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:26<00:00, 11.79it/s]\n     0.227    0.0678    0.0135\n\n     epoch    memory      loss\n     4/100     2.15G     6.908: 100%|█████████| 608/608 [05:00<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:08<00:00, 12.88it/s]\n     0.244     0.104    0.0297\n\n     epoch    memory      loss\n     5/100     1.95G     6.546: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:00<00:00, 13.48it/s]\n     0.208     0.127     0.043\n\n     epoch    memory      loss\n     6/100     1.95G     6.315: 100%|█████████| 608/608 [04:53<00:00,  2.07it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.14it/s]\n     0.217     0.158    0.0567\n\n     epoch    memory      loss\n     7/100      1.9G     6.138: 100%|█████████| 608/608 [05:00<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.247     0.161    0.0652\n\n     epoch    memory      loss\n     8/100     2.04G     6.015: 100%|█████████| 608/608 [04:53<00:00,  2.07it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.42it/s]\n     0.256      0.18    0.0774\n\n     epoch    memory      loss\n     9/100     1.94G     5.896: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:05<00:00, 13.07it/s]\n     0.274      0.19    0.0844\n\n     epoch    memory      loss\n    10/100     1.94G     5.804: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:59<00:00, 13.51it/s]\n     0.286      0.21    0.0949\n\n     epoch    memory      loss\n    11/100     1.94G     5.721: 100%|█████████| 608/608 [05:14<00:00,  1.93it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.20it/s]\n     0.295     0.219     0.102\n\n     epoch    memory      loss\n    12/100     2.04G     5.652: 100%|█████████| 608/608 [04:59<00:00,  2.03it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.39it/s]\n     0.313     0.227     0.111\n\n     epoch    memory      loss\n    13/100     1.87G     5.589: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:00<00:00, 13.48it/s]\n     0.333     0.238     0.121\n\n     epoch    memory      loss\n    14/100     1.71G      5.52: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:59<00:00, 13.55it/s]\n     0.345     0.246     0.128\n\n     epoch    memory      loss\n    15/100     1.97G     5.469: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:58<00:00, 13.61it/s]\n     0.351     0.262     0.138\n\n     epoch    memory      loss\n    16/100     1.83G     5.411: 100%|█████████| 608/608 [05:01<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:59<00:00, 13.53it/s]\n     0.374     0.261     0.144\n\n     epoch    memory      loss\n    17/100     1.95G     5.364: 100%|█████████| 608/608 [04:58<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.25it/s]\n      0.38     0.275     0.151\n\n     epoch    memory      loss\n    18/100        2G     5.317: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.42it/s]\n     0.387     0.279     0.157\n\n     epoch    memory      loss\n    19/100     1.94G     5.289: 100%|█████████| 608/608 [04:55<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:00<00:00, 13.46it/s]\n     0.368      0.29     0.163\n\n     epoch    memory      loss\n    20/100     2.25G     5.263: 100%|█████████| 608/608 [04:57<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.413     0.292     0.171\n\n     epoch    memory      loss\n    21/100     1.81G     5.218: 100%|█████████| 608/608 [04:57<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.28it/s]\n      0.41     0.299     0.177\n\n     epoch    memory      loss\n    22/100     2.08G     5.194: 100%|█████████| 608/608 [05:02<00:00,  2.01it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.18it/s]\n     0.419     0.305     0.183\n\n     epoch    memory      loss\n    23/100     1.84G      5.15: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.28it/s]\n     0.429     0.311     0.188\n\n     epoch    memory      loss\n    24/100      1.9G     5.124: 100%|█████████| 608/608 [05:01<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.33it/s]\n     0.432     0.314     0.193\n\n     epoch    memory      loss\n    25/100     1.81G     5.107: 100%|█████████| 608/608 [04:59<00:00,  2.03it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.25it/s]\n     0.437     0.321     0.199\n\n     epoch    memory      loss\n    26/100     1.93G     5.059: 100%|█████████| 608/608 [05:02<00:00,  2.01it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.18it/s]\n     0.443     0.326     0.203\n\n     epoch    memory      loss\n    27/100     1.95G     5.032: 100%|█████████| 608/608 [05:01<00:00,  2.01it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:00<00:00, 13.45it/s]\n     0.453     0.332     0.209\n\n     epoch    memory      loss\n    28/100      2.1G     5.032: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.40it/s]\n     0.455     0.337     0.212\n\n     epoch    memory      loss\n    29/100     1.96G     4.997: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:00<00:00, 13.44it/s]\n     0.454     0.337     0.216\n\n     epoch    memory      loss\n    30/100     1.99G     4.975: 100%|█████████| 608/608 [05:01<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.29it/s]\n     0.472     0.341     0.221\n\n     epoch    memory      loss\n    31/100     1.87G     4.959: 100%|█████████| 608/608 [05:00<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.21it/s]\n     0.479     0.345     0.224\n\n     epoch    memory      loss\n    32/100     2.07G      4.93: 100%|█████████| 608/608 [05:02<00:00,  2.01it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.464     0.353     0.229\n\n     epoch    memory      loss\n    33/100     2.01G     4.915: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.40it/s]\n     0.486     0.355     0.232\n\n     epoch    memory      loss\n    34/100     1.91G     4.896: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [02:58<00:00, 13.61it/s]\n     0.486      0.36     0.236\n\n     epoch    memory      loss\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.33it/s]\n     0.497      0.36      0.24\n\n     epoch    memory      loss\n    36/100     2.04G     4.857: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.18it/s]\n     0.497     0.366     0.243\n\n     epoch    memory      loss\n    37/100     1.99G     4.842: 100%|█████████| 608/608 [04:58<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.30it/s]\n       0.5     0.368     0.246\n\n     epoch    memory      loss\n    38/100     1.93G     4.835: 100%|█████████| 608/608 [05:03<00:00,  2.00it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.40it/s]\n      0.51     0.371     0.249\n\n     epoch    memory      loss\n    39/100     1.97G      4.81: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.31it/s]\n     0.514     0.372     0.252\n\n     epoch    memory      loss\n    40/100     2.29G     4.787: 100%|█████████| 608/608 [04:58<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.31it/s]\n     0.517     0.374     0.255\n\n     epoch    memory      loss\n    41/100     1.99G     4.783: 100%|█████████| 608/608 [04:59<00:00,  2.03it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.32it/s]\n     0.499      0.38     0.257\n\n     epoch    memory      loss\n    42/100     1.91G     4.771: 100%|█████████| 608/608 [04:58<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.39it/s]\n     0.502     0.381      0.26\n\n     epoch    memory      loss\n    43/100     2.01G     4.747: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.37it/s]\n     0.524     0.381     0.262\n\n     epoch    memory      loss\n    44/100     1.87G     4.736: 100%|█████████| 608/608 [04:54<00:00,  2.07it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.524     0.385     0.264\n\n     epoch    memory      loss\n    45/100     1.82G      4.72: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.32it/s]\n     0.526     0.387     0.266\n\n     epoch    memory      loss\n    46/100     2.07G     4.699: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.32it/s]\n     0.528     0.391     0.269\n\n     epoch    memory      loss\n    47/100     2.02G     4.695: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.39it/s]\n     0.532     0.391     0.271\n\n     epoch    memory      loss\n    48/100     1.87G     4.668: 100%|█████████| 608/608 [04:57<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.33it/s]\n     0.537     0.391     0.273\n\n     epoch    memory      loss\n    49/100     1.91G     4.668: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n    50/100     1.98G     4.639: 100%|█████████| 608/608 [04:54<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.539     0.395     0.276\n\n     epoch    memory      loss\n    51/100        2G     4.623: 100%|█████████| 608/608 [04:55<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.14it/s]\n     0.542     0.395     0.278\n\n     epoch    memory      loss\n    52/100     2.18G      4.62: 100%|█████████| 608/608 [04:58<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:06<00:00, 13.02it/s]\n     0.543     0.397     0.279\n\n     epoch    memory      loss\n    53/100        2G      4.61: 100%|█████████| 608/608 [05:01<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:05<00:00, 13.08it/s]\n     0.545     0.399     0.281\n\n     epoch    memory      loss\n    54/100     1.98G     4.596: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n      0.55     0.399     0.282\n\n     epoch    memory      loss\n    55/100     1.91G     4.589: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.24it/s]\n     0.553     0.401     0.283\n\n     epoch    memory      loss\n    56/100     1.91G     4.573: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.34it/s]\n     0.553     0.402     0.285\n\n     epoch    memory      loss\n    57/100     2.02G     4.566: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.26it/s]\n     0.554     0.403     0.286\n\n     epoch    memory      loss\n    58/100     2.26G     4.545: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.23it/s]\n     0.555     0.405     0.287\n\n     epoch    memory      loss\n    59/100     1.97G      4.55: 100%|█████████| 608/608 [04:57<00:00,  2.04it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.32it/s]\n     0.556     0.406     0.288\n\n     epoch    memory      loss\n    60/100     2.08G     4.529: 100%|█████████| 608/608 [04:54<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:04<00:00, 13.18it/s]\n     0.557     0.407     0.289\n\n     epoch    memory      loss\n    61/100     2.05G     4.516: 100%|█████████| 608/608 [05:02<00:00,  2.01it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.31it/s]\n     0.559     0.407      0.29\n\n     epoch    memory      loss\n    62/100     1.98G     4.502: 100%|█████████| 608/608 [05:00<00:00,  2.02it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:07<00:00, 12.97it/s]\n     0.557     0.409     0.291\n\n     epoch    memory      loss\n    63/100     1.83G      4.49: 100%|█████████| 608/608 [04:58<00:00,  2.03it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.27it/s]\n     0.561     0.408     0.292\n\n     epoch    memory      loss\n    64/100      1.9G      4.48: 100%|█████████| 608/608 [04:57<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.31it/s]\n     0.546      0.41     0.293\n\n     epoch    memory      loss\n    65/100     1.96G     4.461: 100%|█████████| 608/608 [04:55<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.31it/s]\n     0.546     0.412     0.294\n\n     epoch    memory      loss\n    66/100      1.7G     4.459: 100%|█████████| 608/608 [04:55<00:00,  2.06it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:01<00:00, 13.38it/s]\n     0.548     0.413     0.295\n\n     epoch    memory      loss\n    67/100     1.98G     4.448: 100%|█████████| 608/608 [04:56<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:02<00:00, 13.29it/s]\n     0.548     0.414     0.296\n\n     epoch    memory      loss\n    68/100     1.78G     4.435: 100%|█████████| 608/608 [04:57<00:00,  2.05it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:03<00:00, 13.21it/s]\n      0.55     0.415     0.297\n\n     epoch    memory      loss\n    69/100        2G      4.42: 100%|█████████| 608/608 [05:09<00:00,  1.96it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:09<00:00, 12.81it/s]\n     0.549     0.416     0.298\n\n     epoch    memory      loss\n    70/100     2.08G     4.426: 100%|█████████| 608/608 [05:06<00:00,  1.98it/s]\n precision    recall       mAP: 100%|███████| 2430/2430 [03:07<00:00, 12.93it/s]\n     0.548     0.418     0.299\n\n     epoch    memory      loss\n    71/100     1.83G     4.401: 100%|█████████| 608/608 [05:04<00:00,  2.00it/s]\n precision    recall       mAP:  23%|█▊      | 553/2430 [00:43<02:24, 13.03it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":" print('finished train')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T21:48:45.425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 main.py --test","metadata":{"id":"WGAabdTu5oOu","trusted":true,"execution":{"execution_failed":"2025-05-07T21:48:45.426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CUDA_LAUNCH_BLOCKING=1 python train.py --batch-size 16\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T21:48:45.426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}